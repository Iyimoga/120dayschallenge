{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SPLITTING SENTENCES INTO CLAUSES"
      ],
      "metadata": {
        "id": "TfulyL9Xi67k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we work with text, we frequently deal with compound (sentences with two parts\n",
        "that are equally important) and complex sentences (sentences with one part depending\n",
        "on another). It is sometimes useful to split these composite sentences into its component\n",
        "clauses for easier processing down the line. This recipe uses the dependency parse from\n",
        "the previous recipe."
      ],
      "metadata": {
        "id": "6iKISkTHjhRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT THE SPACY PACKAGE"
      ],
      "metadata": {
        "id": "EkTri29Wl5cf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7nIjMRZUi3C5"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD THE SPACY ENGINE"
      ],
      "metadata": {
        "id": "6B06GAmLl-2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "dQ9HeOLwl95L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SET THE SENTENCE TO *He eats cheese, but he won't eat ice cream*:"
      ],
      "metadata": {
        "id": "xIr3zdv6mIaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence =\"He eats cheese, but he won't eat ice cream\""
      ],
      "metadata": {
        "id": "eLnrsmjxmHfq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROCESS THE SENTENCE WITH SPACY ENGINE"
      ],
      "metadata": {
        "id": "safdNVBtmpvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sentence)"
      ],
      "metadata": {
        "id": "S5epR2yamoyX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is instructive to look at the structure of the input sentence by printing out the\n",
        "part of speech, dependency tag, ancestors, and children of each token. This can be\n",
        "accomplished using the following code:"
      ],
      "metadata": {
        "id": "EJGWR_sCm30O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  ancestors = [t.text for t in token.ancestors]\n",
        "  children = [t.text for t in token.children]\n",
        "  print(token.text, \"\\t\", token.i, \"\\t\",\n",
        "        token.pos_, \"\\t\", token.dep_, \"\\t\",\n",
        "        ancestors, \"\\t\", children)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGJhW49Pmw4O",
        "outputId": "a610f284-c7e4-4a38-ac8d-6ede3391112d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He \t 0 \t PRON \t nsubj \t ['eats'] \t []\n",
            "eats \t 1 \t VERB \t ROOT \t [] \t ['He', 'cheese', ',', 'but', 'eat']\n",
            "cheese \t 2 \t NOUN \t dobj \t ['eats'] \t []\n",
            ", \t 3 \t PUNCT \t punct \t ['eats'] \t []\n",
            "but \t 4 \t CCONJ \t cc \t ['eats'] \t []\n",
            "he \t 5 \t PRON \t nsubj \t ['eat', 'eats'] \t []\n",
            "wo \t 6 \t AUX \t aux \t ['eat', 'eats'] \t []\n",
            "n't \t 7 \t PART \t neg \t ['eat', 'eats'] \t []\n",
            "eat \t 8 \t VERB \t conj \t ['eats'] \t ['he', 'wo', \"n't\", 'cream']\n",
            "ice \t 9 \t NOUN \t compound \t ['cream', 'eat', 'eats'] \t []\n",
            "cream \t 10 \t NOUN \t dobj \t ['eat', 'eats'] \t ['ice']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the following function to fnd the root token of the sentence, which is\n",
        "usually the main verb. In instances where there is a dependent clause, it is the verb\n",
        "of the independent clause:"
      ],
      "metadata": {
        "id": "ASFyCtJCn71b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_root_of_sentence(doc):\n",
        "  root_token = None\n",
        "  for token in doc:\n",
        "    if (token.dep_ == \"ROOT\"):\n",
        "      root_token = token\n",
        "  return root_token"
      ],
      "metadata": {
        "id": "S5B8DRtgnjt2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now fnd the root token of the sentence:"
      ],
      "metadata": {
        "id": "p96dUUo9obnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_token = find_root_of_sentence(doc)"
      ],
      "metadata": {
        "id": "T2lG8LuhoT12"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the following function to fnd the other verbs in the sentence:"
      ],
      "metadata": {
        "id": "SYd8TqDaopCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_other_verbs(doc, root_token):\n",
        "  other_verbs = []\n",
        "  for token in doc:\n",
        "    ancestors = list(token.ancestors)\n",
        "    if (token.pos_ == \"VERB\" and len(ancestors) == 1\\\n",
        "        and ancestors[0] == root_token):\n",
        "      other_verbs.append(token)\n",
        "  return other_verbs"
      ],
      "metadata": {
        "id": "zJ_CeZyeoiu1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the preceding function to fnd the remaining verbs in the sentence:"
      ],
      "metadata": {
        "id": "FD7_FkTUpbrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "other_verbs = find_other_verbs(doc, root_token)"
      ],
      "metadata": {
        "id": "fF3W_nfKpXc7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the following function to fnd the token spans for each verb:"
      ],
      "metadata": {
        "id": "o8MX1RjOpxiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clause_token_span_for_verb(verb, doc, all_verbs):\n",
        "  first_toekn_index = len(doc)\n",
        "  last_token_index = 0\n",
        "  this_verb_children = list(verb.children)\n",
        "  for child in this_verb_children:\n",
        "    if (child not in all_verbs):\n",
        "      first_token_index = child.i\n",
        "    if (child.i > last_token_index):\n",
        "      last_token_index = child.i\n",
        "  return(first_token_index, last_token_index)"
      ],
      "metadata": {
        "id": "aBr-KHxGp2_F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will put together all the verbs in one array and process each using the preceding\n",
        "function. This will return a tuple of start and end indices for each verb's clause:"
      ],
      "metadata": {
        "id": "qJsp0Yhxq4nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_spans = []\n",
        "all_verbs = [root_token] + other_verbs\n",
        "for other_verb in all_verbs:\n",
        "  (first_token_index, last_token_index) = \\\n",
        "  get_clause_token_span_for_verb(other_verb,\n",
        "                                 doc, all_verbs)\n",
        "  token_spans.append((first_token_index, last_token_index))"
      ],
      "metadata": {
        "id": "xMWmS1YApwtB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the start and end indices, we can now put together token spans for each\n",
        "clause. We sort the sentence_clauses list at the end so that the clauses are in\n",
        "the order they appear in the sentence:"
      ],
      "metadata": {
        "id": "UpsqGM3Wr2XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_clauses = []\n",
        "for token_span in token_spans:\n",
        "  start = token_span[0]\n",
        "  end = token_span[1]\n",
        "  if (start < end):\n",
        "    clause = doc[start:end]\n",
        "    sentence_clauses.append(clause)\n",
        "sentence_clauses = sorted(sentence_clauses, key = lambda tup: tup[0])"
      ],
      "metadata": {
        "id": "TKE_OYaIrwlm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\Now, we can print the fnal result of the processing for our initial sentence; that is,\n",
        "He eats cheese, but he won't eat ice cream:"
      ],
      "metadata": {
        "id": "6Zvas6lfs3io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clauses_text = [clause.text for clause in sentence_clauses]\n",
        "print(clauses_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIPH3LSasyl4",
        "outputId": "c2e4789f-1785-4e7d-eda8-f68236a4eab8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"but he won't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How it worksâ€¦\n",
        "The way the code works is based on the way complex and compound sentences are\n",
        "structured. Each clause contains a verb, and one of the verbs is the main verb of the\n",
        "sentence (root). The code looks for the root verb, always marked with the ROOT\n",
        "dependency tag in spaCy processing, and then looks for the other verbs in the sentence.\n",
        "The code then uses the information about each verb's children to fnd the lef and right\n",
        "boundaries of the clause. Using this information, the code then constructs the text of the\n",
        "clauses. A step-by-step explanation follows.\n",
        "In step 1, we import the spaCy package and in step 2, we load the spacy engine. In step\n",
        "3, we set the sentence variable and in step 4, we process it using the spacy engine. In step\n",
        "5, we print out the dependency parse information. It will help us determine how to split\n",
        "the sentence into clauses.\n",
        "In step 6, we defne the find_root_of_sentence function, which returns the token\n",
        "that has a dependency tag of ROOT. In step 7, we fnd the root of the sentence we are using\n",
        "as an example.\n",
        "In step 8, we defne the find_other_verbs function, which will fnd other verbs in the\n",
        "sentence. In this function, we look for tokens that have the VERB part of speech tag and\n",
        "has the root token as its only ancestor. In step 9, we apply this function.\n",
        "In step 10, we defne the get_clause_token_span_for_verb function, which will\n",
        "fnd the beginning and ending index for the verb. Te function goes through all the verb's\n",
        "children; the lefmost child's index is the beginning index, while the rightmost child's\n",
        "index is the ending index for this verb's clause.\n",
        "EBSCOhost - printed on 2/9/2023 7:36 AM via . All use subject to https://www.ebsco.com/terms-of-useExtracting noun chunks 41\n",
        "In step 11, we use the preceding function to fnd the clause indices for each verb. The\n",
        "token_spans variable contains the list of tuples, where the frst tuple element is the\n",
        "beginning clause index and the second tuple element is the ending clause index.\n",
        "In step 12, we create token Span objects for each clause in the sentence using the list of\n",
        "beginning and ending index pairs we created in step 11. We get the Span object by slicing\n",
        "the Doc object and then appending the resulting Span objects to a list. As a fnal step,\n",
        "we sort the list to make sure that the clauses in the list are in the same order as in the\n",
        "sentence.\n",
        "In step 13, we print the clauses in our sentence. You will notice that the word but is\n",
        "missing, since its parent is the root verb eats, although it appears in the other clause. Te\n",
        "exercise of including but is lef to you."
      ],
      "metadata": {
        "id": "O1XwKLt9tcni"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RlWEjKCOtFNV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}