{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Representing texts with TF-IDF"
      ],
      "metadata": {
        "id": "CDxqxNzyngj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go one step further and use the TF-IDF algorithm to count words and ngrams in\n",
        "incoming documents. TF-IDF stands for term frequency-inverse document frequency\n",
        "and gives more weight to words that are unique to a document than to words that are\n",
        "frequent, but repeated throughout most documents. This allows us to give more weight\n",
        "to words uniquely characteristic to particular documents."
      ],
      "metadata": {
        "id": "z4PmdpJkn4LH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this recipe, we will use a diﬀerent type of vectorizer that can apply the TF-IDF\n",
        "algorithm to the input text. Like the CountVectorizer class, it has an analyzer that we\n",
        "will use to show the representations of new sentences."
      ],
      "metadata": {
        "id": "00yjNysHoCJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting ready\n",
        "We will be using the TfidfVectorizer class from the sklearn package."
      ],
      "metadata": {
        "id": "Bj0H6HyxoMch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to do it…\n",
        "Te TfidfVectorizer class allows for all the functionality of CountVectorizer,\n",
        "except that it uses the TF-IDF algorithm to count the words instead of direct counts. The\n",
        "other features of the class should be familiar."
      ],
      "metadata": {
        "id": "4fOPJe8goY-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MPORT LIBRARIES"
      ],
      "metadata": {
        "id": "4EXVciEuS_iQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dv7UfEqwnYgb"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v91s9QD7ToMB",
        "outputId": "362440f7-1cae-4300-92fd-c31068ef064b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "READ IN THE BOOK TEXT(corpus)"
      ],
      "metadata": {
        "id": "niWSUI1CVJcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "filename = \"002_Sign_of_Four.txt\"\n",
        "file = open(r\"/content/002_Sign_of_Four.txt\", \"r\", encoding = \"UTF-8\")\n",
        "text = file.read()\n",
        "\n",
        "text = text.replace(\"\\n\", \" \")\n",
        "# Initialize an NLTK tokenizer. Tis uses the punkt model we downloaded\n",
        "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
        "\n",
        "# Divide the text into sentences:\n",
        "sentences = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GTyV3cvU25N",
        "outputId": "9ca32796-9f44-408e-ddee-652d46cf1983"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INITIALIZE THE Snowball Stemmer(english)"
      ],
      "metadata": {
        "id": "XHGgc3rMV8Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "ztrVkh37V24a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GET THE SET OF ENGLISH stopwords"
      ],
      "metadata": {
        "id": "FH3eK_FiWQsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_stopwords = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "f7AWA1ijWPpJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMMING AND TOKENIZATION"
      ],
      "metadata": {
        "id": "e_VXOv_4WvSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemmer_tokenizer(text):\n",
        "  text = text.translate(str.maketrans(\"\", \"\", string.punctuation)).lower() # removes punctuation and converts to lowercase\n",
        "\n",
        "  words = re.findall(r\"\\b\\w+\\b\", text) # tokenization\n",
        "\n",
        "  stemmed_words = [\n",
        "      stemmer.stem(word)\n",
        "      for word in words if word not in english_stopwords\n",
        "  ]\n",
        "  return stemmed_words"
      ],
      "metadata": {
        "id": "f9Ll4rIuWjwx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INITIALIZE THE VECTORIZER"
      ],
      "metadata": {
        "id": "XEAIJnu2Y0iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    tokenizer = stemmer_tokenizer, analyzer = \"word\"\n",
        ")"
      ],
      "metadata": {
        "id": "ruqc3mVTYle3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FIT_TRANSFORM THE corpus"
      ],
      "metadata": {
        "id": "YA-313XQZjMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_tfidf_sparse = tfidf_vectorizer.fit_transform(sentences)\n",
        "x_tfidf_dense = x_tfidf_sparse.toarray()\n",
        "features = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "print(f\"Total Features: {len(features)}\")\n",
        "print(f\"Features: {features}\")"
      ],
      "metadata": {
        "id": "NVQ2Js-CZhwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5365dd-7408-43d6-f7ca-6e078ebb412f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Features: 4168\n",
            "Features: ['10the' '11the' '12the' ... 'youwould' 'zigzag' 'zum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESULTS"
      ],
      "metadata": {
        "id": "ISSnRTOdc_r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_tfidf = pd.DataFrame(x_tfidf_dense,\n",
        "                          columns=features,\n",
        "                          index=[f\"Document {i+1}\" for i in range(len(sentences))])\n",
        "\n",
        "print(f\"TF_IDF Weight Matrix\")\n",
        "print(data_tfidf)"
      ],
      "metadata": {
        "id": "oB33mRNZawJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ce297a-d181-400b-8586-5fb9b8a90394"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF_IDF Weight Matrix\n",
            "               10the  11the  12the  1857  1871  1878  1878near  1882  1882an  \\\n",
            "Document 1       0.0    0.0    0.0   0.0   0.0   0.0       0.0   0.0     0.0   \n",
            "Document 2       0.0    0.0    0.0   0.0   0.0   0.0       0.0   0.0     0.0   \n",
            "Document 3       0.0    0.0    0.0   0.0   0.0   0.0       0.0   0.0     0.0   \n",
            "Document 4       0.0    0.0    0.0   0.0   0.0   0.0       0.0   0.0     0.0   \n",
            "Document 5       0.0    0.0    0.0   0.0   0.0   0.0       0.0   0.0     0.0   \n",
            "...              ...    ...    ...   ...   ...   ...       ...   ...     ...   \n",
            "Document 2921    0.0    0.0    0.0   0.0   0.0   0.0       0.0   0.0     0.0   \n",
            "Document 2922    0.0    0.0    0.0   0.0   0.0   0.0       0.0   0.0     0.0   \n",
            "Document 2923    0.0    0.0    0.0   0.0   0.0   0.0       0.0   0.0     0.0   \n",
            "Document 2924    0.0    0.0    0.0   0.0   0.0   0.0       0.0   0.0     0.0   \n",
            "Document 2925    0.0    0.0    0.0   0.0   0.0   0.0       0.0   0.0     0.0   \n",
            "\n",
            "                   1the  ...  yield  yonder  youfor  youll  young  your  \\\n",
            "Document 1     0.289367  ...    0.0     0.0     0.0    0.0    0.0   0.0   \n",
            "Document 2     0.000000  ...    0.0     0.0     0.0    0.0    0.0   0.0   \n",
            "Document 3     0.000000  ...    0.0     0.0     0.0    0.0    0.0   0.0   \n",
            "Document 4     0.000000  ...    0.0     0.0     0.0    0.0    0.0   0.0   \n",
            "Document 5     0.000000  ...    0.0     0.0     0.0    0.0    0.0   0.0   \n",
            "...                 ...  ...    ...     ...     ...    ...    ...   ...   \n",
            "Document 2921  0.000000  ...    0.0     0.0     0.0    0.0    0.0   0.0   \n",
            "Document 2922  0.000000  ...    0.0     0.0     0.0    0.0    0.0   0.0   \n",
            "Document 2923  0.000000  ...    0.0     0.0     0.0    0.0    0.0   0.0   \n",
            "Document 2924  0.000000  ...    0.0     0.0     0.0    0.0    0.0   0.0   \n",
            "Document 2925  0.000000  ...    0.0     0.0     0.0    0.0    0.0   0.0   \n",
            "\n",
            "               youth  youwould  zigzag  zum  \n",
            "Document 1       0.0       0.0     0.0  0.0  \n",
            "Document 2       0.0       0.0     0.0  0.0  \n",
            "Document 3       0.0       0.0     0.0  0.0  \n",
            "Document 4       0.0       0.0     0.0  0.0  \n",
            "Document 5       0.0       0.0     0.0  0.0  \n",
            "...              ...       ...     ...  ...  \n",
            "Document 2921    0.0       0.0     0.0  0.0  \n",
            "Document 2922    0.0       0.0     0.0  0.0  \n",
            "Document 2923    0.0       0.0     0.0  0.0  \n",
            "Document 2924    0.0       0.0     0.0  0.0  \n",
            "Document 2925    0.0       0.0     0.0  0.0  \n",
            "\n",
            "[2925 rows x 4168 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How it works…\n",
        "The TfidfVectorizer class works almost exactly like the CountVectorizer class,\n",
        "diﬀering only in the way the word frequencies are calculated, so most of the steps should\n",
        "be familiar here. Word frequencies are calculated as follows. For each word, the overall\n",
        "frequency is a product of the term frequency and the inverse document frequency. Term\n",
        "frequency is the number of times the word occurs in the document. Inverse document\n",
        "frequency is the total number of documents divided by the number of documents where\n",
        "the word occurs. Usually, these frequencies are logarithmically scaled."
      ],
      "metadata": {
        "id": "vzRomEYol734"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ykPIVOxAfidK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}