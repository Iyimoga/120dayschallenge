{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing the N-gram model"
      ],
      "metadata": {
        "id": "ofhJoPzJqm0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representing a document as a bag of words is useful, but semantics is about more than\n",
        "just words in isolation. To capture word combinations, an n-gram model is useful. Its\n",
        "vocabulary consists not just of words, but word sequences, or n-grams. We will build a\n",
        "bigram model in this recipe, where bigrams are sequences of two words."
      ],
      "metadata": {
        "id": "laKfXyM3rLdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting ready\n",
        "Te CountVectorizer class is very versatile and allows us to construct n-gram models.\n",
        "We will use it again in this recipe. We will also explore how to build character n-gram\n",
        "models using this class."
      ],
      "metadata": {
        "id": "Xk76xseWrUme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download(\"punkt_tab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Prz32tGQ6301",
        "outputId": "26bb9103-f1cd-4180-9c73-f6b1f192a8fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_tfXRtWbnRNI"
      },
      "outputs": [],
      "source": [
        "# Read in the book text(corpus):\n",
        "filename = \"001_Study_in_Scarlet.txt\"\n",
        "file = open(r\"/content/001_Study_in_Scarlet.txt\", \"r\", encoding=\"utf-8\")\n",
        "text = file.read()\n",
        "# Replace newlines with spaces:\n",
        "text = text.replace(\"\\n\", \" \")\n",
        "# Initialize an NLTK tokenizer. Tis uses the punkt model we downloaded\n",
        "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
        "\n",
        "# Divide the text into sentences:\n",
        "sentences = tokenizer.tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INITIALIZE AND CONFIGURATION FOR BIGRAMS"
      ],
      "metadata": {
        "id": "Q7bAYD_J7c47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIALIZE COUNTVECTORIZER FOR BIGRAM\n",
        "bigram = CountVectorizer(ngram_range = (2, 2))"
      ],
      "metadata": {
        "id": "wDoMDvXQ7LI6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIT AND TRANSFORM THE CORPUS\n",
        "x_bigram_sparse = bigram.fit_transform(sentences)"
      ],
      "metadata": {
        "id": "gPPGpKWb8bJC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONVERT TO DENSE ARRAY FOR VIEWING\n",
        "x_bigram_dense = x_bigram_sparse.toarray()"
      ],
      "metadata": {
        "id": "DjdPg1Y289rT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSPECT THE BIGRAMS(FEATURES)"
      ],
      "metadata": {
        "id": "NWYpdQJz9RVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_features = bigram.get_feature_names_out()\n",
        "print(f\"Learned Bigrams(features): \")\n",
        "print(f\"Total Unique Bigrams: {len(bigram_features)}\")\n",
        "print(bigram_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0bpUz5o9PDK",
        "outputId": "cdbbaa82-c614-4611-877c-d50a6c666c66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Bigrams(features): \n",
            "Total Unique Bigrams: 24287\n",
            "['10 plays' '11 is' '12 has' ... 'youth thinking' 'youth with' 'zeal for']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DISPLAY THE BIGRAM COUNT MATRIX"
      ],
      "metadata": {
        "id": "Qg-BcQsH-Voz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_bigrams = pd.DataFrame(x_bigram_dense,\n",
        "                           columns = bigram_features,\n",
        "                            index = [f\"Doc {i + 1}\" for i in range(len(sentences))])\n",
        "print(f\"Bigram Count Matrix (C(w_i-1, w_i))\")\n",
        "print(data_bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U11CSpK49oU9",
        "outputId": "fb66123a-be67-4537-efb6-af8f028bae9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Count Matrix (C(w_i-1, w_i))\n",
            "          10 plays  11 is  12 has  129 camberwell  13 duncan  13 we  15 and  \\\n",
            "Doc 1            0      0       0               0          0      0       0   \n",
            "Doc 2            0      0       0               0          0      0       0   \n",
            "Doc 3            0      0       0               0          0      0       0   \n",
            "Doc 4            0      0       0               0          0      0       0   \n",
            "Doc 5            0      0       0               0          0      0       0   \n",
            "...            ...    ...     ...             ...        ...    ...     ...   \n",
            "Doc 2671         0      0       0               0          0      0       0   \n",
            "Doc 2672         0      0       0               0          0      0       0   \n",
            "Doc 2673         0      0       0               0          0      0       0   \n",
            "Doc 2674         0      0       0               0          0      0       0   \n",
            "Doc 2675         0      0       0               0          0      0       0   \n",
            "\n",
            "          1878 took  221b baker  27 had  ...  yourself contented  yourself he  \\\n",
            "Doc 1             1           0       0  ...                   0            0   \n",
            "Doc 2             0           0       0  ...                   0            0   \n",
            "Doc 3             0           0       0  ...                   0            0   \n",
            "Doc 4             0           0       0  ...                   0            0   \n",
            "Doc 5             0           0       0  ...                   0            0   \n",
            "...             ...         ...     ...  ...                 ...          ...   \n",
            "Doc 2671          0           0       0  ...                   0            0   \n",
            "Doc 2672          0           0       0  ...                   0            0   \n",
            "Doc 2673          0           0       0  ...                   0            0   \n",
            "Doc 2674          0           0       0  ...                   0            0   \n",
            "Doc 2675          0           0       0  ...                   1            0   \n",
            "\n",
            "          yourself in  yourself interrupted  yourself my  yourself watson  \\\n",
            "Doc 1               0                     0            0                0   \n",
            "Doc 2               0                     0            0                0   \n",
            "Doc 3               0                     0            0                0   \n",
            "Doc 4               0                     0            0                0   \n",
            "Doc 5               0                     0            0                0   \n",
            "...               ...                   ...          ...              ...   \n",
            "Doc 2671            0                     0            0                0   \n",
            "Doc 2672            0                     0            0                0   \n",
            "Doc 2673            0                     0            0                0   \n",
            "Doc 2674            0                     0            0                0   \n",
            "Doc 2675            0                     0            0                0   \n",
            "\n",
            "          youth of  youth thinking  youth with  zeal for  \n",
            "Doc 1            0               0           0         0  \n",
            "Doc 2            0               0           0         0  \n",
            "Doc 3            0               0           0         0  \n",
            "Doc 4            0               0           0         0  \n",
            "Doc 5            0               0           0         0  \n",
            "...            ...             ...         ...       ...  \n",
            "Doc 2671         0               0           0         0  \n",
            "Doc 2672         0               0           0         0  \n",
            "Doc 2673         0               0           0         0  \n",
            "Doc 2674         0               0           0         0  \n",
            "Doc 2675         0               0           0         0  \n",
            "\n",
            "[2675 rows x 24287 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CALCULATE CONDITIONAL PROBABILITIES"
      ],
      "metadata": {
        "id": "QdX9meRo_r6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_vectorizer = CountVectorizer(ngram_range = (1, 1))\n",
        "x_unigram_sparse = unigram_vectorizer.fit_transform(sentences)\n",
        "\n",
        "x_unigram_dense = x_unigram_sparse.toarray()\n",
        "\n",
        "unigram_features = unigram_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "9S-t4IWY_Ws3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATAFRAME FOR THE UNIGRAM COUNTS\n",
        "data_unigram = pd.DataFrame(x_unigram_dense,\n",
        "    columns = unigram_features,\n",
        "    index = [f\"Doc {i + 1}\" for i in range(len(sentences))])\n",
        "\n",
        "print(f\"Unigram Count Matrix (C(w_i-1))\")\n",
        "\n",
        "print(data_unigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfv41XN8BLZw",
        "outputId": "236a2227-5ddc-4339-c3a3-83897201ad17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Count Matrix (C(w_i-1))\n",
            "          10  11  12  129  13  15  1642  1878  221b  27  ...  youngest  \\\n",
            "Doc 1      0   0   0    0   0   0     0     1     0   0  ...         0   \n",
            "Doc 2      0   0   0    0   0   0     0     0     0   0  ...         0   \n",
            "Doc 3      0   0   0    0   0   0     0     0     0   0  ...         0   \n",
            "Doc 4      0   0   0    0   0   0     0     0     0   0  ...         0   \n",
            "Doc 5      0   0   0    0   0   0     0     0     0   0  ...         0   \n",
            "...       ..  ..  ..  ...  ..  ..   ...   ...   ...  ..  ...       ...   \n",
            "Doc 2671   0   0   0    0   0   0     0     0     0   0  ...         0   \n",
            "Doc 2672   0   0   0    0   0   0     0     0     0   0  ...         0   \n",
            "Doc 2673   0   0   0    0   0   0     0     0     0   0  ...         0   \n",
            "Doc 2674   0   0   0    0   0   0     0     0     0   0  ...         0   \n",
            "Doc 2675   0   0   0    0   0   0     0     0     0   0  ...         0   \n",
            "\n",
            "          youngster  youngsters  your  yours  yourself  youth  youths  zeal  \\\n",
            "Doc 1             0           0     0      0         0      0       0     0   \n",
            "Doc 2             0           0     0      0         0      0       0     0   \n",
            "Doc 3             0           0     0      0         0      0       0     0   \n",
            "Doc 4             0           0     0      0         0      0       0     0   \n",
            "Doc 5             0           0     0      0         0      0       0     0   \n",
            "...             ...         ...   ...    ...       ...    ...     ...   ...   \n",
            "Doc 2671          0           0     0      0         0      0       0     0   \n",
            "Doc 2672          0           0     0      0         0      0       0     0   \n",
            "Doc 2673          0           0     0      0         0      0       0     0   \n",
            "Doc 2674          0           0     0      0         0      0       0     0   \n",
            "Doc 2675          0           0     0      0         1      0       0     0   \n",
            "\n",
            "          zion  \n",
            "Doc 1        0  \n",
            "Doc 2        0  \n",
            "Doc 3        0  \n",
            "Doc 4        0  \n",
            "Doc 5        0  \n",
            "...        ...  \n",
            "Doc 2671     0  \n",
            "Doc 2672     0  \n",
            "Doc 2673     0  \n",
            "Doc 2674     0  \n",
            "Doc 2675     0  \n",
            "\n",
            "[2675 rows x 5653 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# There's moreâ€¦\n",
        "We can use trigrams, quadrigrams, and more in the vectorizer by providing the\n",
        "corresponding tuple to the ngram_range argument. The downside of this is the everexpanding vocabulary and the growth of sentence vectors, since each sentence vector has\n",
        "to have an entry for each word in the input vocabulary."
      ],
      "metadata": {
        "id": "6-tbqRNKED8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXjQgMlUCJC6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}