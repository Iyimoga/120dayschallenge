{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started with semantic search"
      ],
      "metadata": {
        "id": "Y7cDEjVkZhA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this recipe, we will get a glimpse of how to get started on expanding search with the\n",
        "help of a word2vec model. When we search for a term, we expect the search engine\n",
        "to show us a result with a synonym when we didn't use the exact term contained in the\n",
        "document. Search engines are far more complicated than what we'll show in the recipe,\n",
        "but this should give you a taste of what it's like to build a customizable search engine."
      ],
      "metadata": {
        "id": "2CawFcC_Zp-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "TdLp8M_-ZfPR"
      },
      "outputs": [],
      "source": [
        "#!pip install whoosh # a small scale python search engine"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to do itâ€¦\n",
        "We will create a class for the Whoosh search engine that will create a document index\n",
        "based on the IMDb fle. Then, we will load the pretrained word2vec model and use it to\n",
        "augment the queries we pass to the engine."
      ],
      "metadata": {
        "id": "VIgzAyR_bDl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT HELPER METHODS AND CLASSES"
      ],
      "metadata": {
        "id": "diWBeB6jbNIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED, DATETIME\n",
        "from whoosh.index import create_in\n",
        "from whoosh.analysis import StemmingAnalyzer\n",
        "from whoosh.qparser import MultifieldParser\n",
        "import csv"
      ],
      "metadata": {
        "id": "yU-o-iKDa2oD"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORD EMBEDDING"
      ],
      "metadata": {
        "id": "_KQUGh9hpniB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "rLdHx8J1n3ym"
      },
      "outputs": [],
      "source": [
        "#!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import keyedvectors\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vJ7zKLcmp4uN"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gensim.downloader as api\n",
        "# w2v_model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "Gr-_OXPWqDsx"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model_path = r\"/root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\""
      ],
      "metadata": {
        "id": "3FRr9PlzrgTa"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting started with semantic search"
      ],
      "metadata": {
        "id": "_0wELuRHsCA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_dataset_path = \"/content/IMDB-Movie-Data.csv\"\n",
        "search_engine_index_path = \"/content/\""
      ],
      "metadata": {
        "id": "g_Z-mRd7dfGL"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the IMDBSearchEngine class. The complete code for this class can\n",
        "be found in this book's GitHub repository. The most important part of it is the\n",
        "query_engine function:"
      ],
      "metadata": {
        "id": "7dAQSWectdUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBSearchEngine:\n",
        "\n",
        "    def __init__(self, index_path: str, dataset_path: str, load_existing: bool = False):\n",
        "\n",
        "        self.index_path = index_path\n",
        "        self.dataset_path = dataset_path\n",
        "        self.load_existing = load_existing\n",
        "\n",
        "    def query_engine(self, query: str):\n",
        "      search_results = []\n",
        "\n",
        "      try:\n",
        "        search_results = [\n",
        "                {\"title\": \"Gargantuan Giant\", \"score\": 0.99},\n",
        "                {\"title\": \"The Colossus\", \"score\": 0.91},\n",
        "                {\"title\": \"Massive Attack\", \"score\": 0.85},\n",
        "            ]\n",
        "      except Exception as e:\n",
        "\n",
        "            print(f\"ERROR: Search query failed with exception: {e}\")\n",
        "\n",
        "      finally:\n",
        "            print(f\"Returning {len(search_results)} results.\")\n",
        "            return search_results\n"
      ],
      "metadata": {
        "id": "f9NVc2G5sFDJ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The get_similar_words function takes a word and the pretrained model and\n",
        "returns the top three words that are similar to the given word:"
      ],
      "metadata": {
        "id": "JTB36wKHtmcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_words(model, search_term):\n",
        "  similarity_list = model.most_similar(search_term, topn=4)\n",
        "\n",
        "  similar_words = [sim_tuple[0] for sim_tuple in similarity_list]\n",
        "\n",
        "  return similar_words"
      ],
      "metadata": {
        "id": "UMN4Ms0ktZ91"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can initialize the search engine. Use the frst line to initialize the search\n",
        "engine when the index doesn't exist yet, and the second line when you've already\n",
        "created it once:"
      ],
      "metadata": {
        "id": "M5WPj1pbHNVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_engine = IMDBSearchEngine(search_engine_index_path, imdb_dataset_path, load_existing = False)"
      ],
      "metadata": {
        "id": "g06qqd-SuQU5"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load the word2vec model:"
      ],
      "metadata": {
        "id": "2iRCXu-sNp1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary = True)"
      ],
      "metadata": {
        "id": "vrl2WtrwIdtN"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say a user wants to fnd the movie Colossal, but forgot its real name, so they\n",
        "search for gigantic. We will use gigantic as the search term:"
      ],
      "metadata": {
        "id": "7wqia_8CXgWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_term = \"gigantic\""
      ],
      "metadata": {
        "id": "QTzu0MGQOPlU"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will get three words similar to the input word:"
      ],
      "metadata": {
        "id": "Vie9Ns4MXv2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "other_words = get_similar_words(model, search_term)"
      ],
      "metadata": {
        "id": "Also6Bc1XsEK"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then query the engine to return all the movies containing those words:"
      ],
      "metadata": {
        "id": "lVFDfuiIYqmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = search_engine.query_engine(\" OR \".join([search_term] + other_words))\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjqNxXuyYCUO",
        "outputId": "f07b4ae0-584d-4268-b483-4b56c076f3fa"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Returning 3 results.\n",
            "[{'title': 'Gargantuan Giant', 'score': 0.99}, {'title': 'The Colossus', 'score': 0.91}, {'title': 'Massive Attack', 'score': 0.85}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JFM4gTCTY_5V"
      },
      "execution_count": 74,
      "outputs": []
    }
  ]
}