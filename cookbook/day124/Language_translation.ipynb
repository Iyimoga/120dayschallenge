{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Language translation"
      ],
      "metadata": {
        "id": "1xZrICDXJ13N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this recipe, we will use transformers for language translation. We will use the **Google Text-To-Text Transfer Transformer (T5)** model. This model is an end-to-end model that uses both the encoder and decoder components of the transformer model."
      ],
      "metadata": {
        "id": "2XjD0SoAKEfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to do it...\n",
        "\n",
        "In this recipe, you will initialize a seed sentence in English and translate it to French. The T5 model expects the input format to encode the information about the language translation task along with the seed sentence. In this case, the encoder uses the input in the source language and generates a representation of the text. The decoder uses this representation and generates text for the target language. The T5 model is trained specifically for this task, in addition to many others."
      ],
      "metadata": {
        "id": "gOCnBnnfL5WB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "LdnJC2WhMeNh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7bdZ1atrJdfI"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    T5Tokenizer, T5ForConditionalGeneration\n",
        ")\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize a tokenizer and model instance with the t5-base model from Google. We use the model_max_length parameter of 200"
      ],
      "metadata": {
        "id": "FSWEl9aZNMJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\n",
        "    \"t5-base\", model_max_length = 200\n",
        ")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "    \"t5-base\", return_dict = True\n",
        ")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "PI9Fb4OOM19W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize a seed sequence that you want to translate:"
      ],
      "metadata": {
        "id": "mzZ8tmtrOed5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_sequence = (\"It's such a beautiful morning today!\")"
      ],
      "metadata": {
        "id": "AfqvKaOJN_5d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the input sequence. The tokenizer specifies the source and the target language as part of its input encoding. This is done by appending the “translate English to French:” text to the input seed sequence. We load these token IDs into the device that is used for computation. It is a requirement for both the model and the token IDs to be on the same device:"
      ],
      "metadata": {
        "id": "QKEgRvk-O1VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(\n",
        "    \"translate English to French: \" + language_sequence,\n",
        "    return_tensors = \"pt\",\n",
        "    truncation = True).input_ids.to(device)"
      ],
      "metadata": {
        "id": "wi3gw5yLOpVZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translate the source language token IDs to the target language token IDs via the model. The model uses the encoder-decoder architecture to convert the input token IDs to the output token IDs:"
      ],
      "metadata": {
        "id": "jHA13VqWPdHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_ids = model.generate(input_ids,\n",
        "                             max_new_tokens = 200)"
      ],
      "metadata": {
        "id": "0l25iEOGPW4D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decode the text from the token IDs to the target language tokens. We use the tokenizer to convert the output token IDs to the target language tokens:"
      ],
      "metadata": {
        "id": "Zy0J-0aVP1zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_translation = tokenizer.decode(language_ids[0],\n",
        "                                        skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "V4ZQleAxPsw1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the translated output:"
      ],
      "metadata": {
        "id": "C7WwpSS1QI3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(language_translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aeDBraWQGnw",
        "outputId": "b22836ef-1d42-4fa2-c927-7bd4f439fdaa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C'est un beau matin aujourd'hui!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qAai5-hQP-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}