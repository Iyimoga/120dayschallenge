{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572531a9-687b-4340-ba12-4b7599d70cf1",
   "metadata": {},
   "source": [
    "# CREATING AN INVERSE IMAGE SEARCH INDEX WITH DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9742d610-e588-42b0-9ed0-27602f074385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf711c30-1698-427f-ad89-389a5278a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b4186c-db6b-4ac5-98cd-d496ba0b30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.datasets import fashion_mnist \n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597081f-b84f-4c7f-a30f-11b1b1677960",
   "metadata": {},
   "source": [
    "DEFINE BUID_AUTOENCODER(), WHICH INSTANTIATES THE AUTOENCODER.\n",
    "FIRST, LET'S ASSEMBLE THE ENCODER PART:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f47b40-7c4a-4af1-94a4-4463c7ced039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_shape = (28, 28, 1), \n",
    "                      encoding_size = 32,\n",
    "                     negative_slope = 0.3):\n",
    "    inputs = Input(shape = input_shape)\n",
    "    encoder = Conv2D(filters = 32,\n",
    "                    kernel_size = (3, 3),\n",
    "                    strides = 2,\n",
    "                    padding = \"same\") (inputs)\n",
    "    encoder = LeakyReLU(negative_slope = negative_slope) (encoder)\n",
    "    encoder = BatchNormalization() (encoder)\n",
    "    encoder = Conv2D(filters = 64,\n",
    "                    kernel_size = (3, 3),\n",
    "                    strides = 2,\n",
    "                    padding = \"same\") (encoder)\n",
    "    encoder = LeakyReLU(negative_slope = negative_slope) (encoder)\n",
    "    encoder = BatchNormalization() (encoder)\n",
    "\n",
    "    encoder_output_shape = encoder.shape\n",
    "    encoder = Flatten() (encoder)\n",
    "    encoder_output = Dense(units = encoding_size,\n",
    "                          name = \"encoder_output\") (encoder)\n",
    "\n",
    "\n",
    "\n",
    "    #DEFINE THE DECODER PORTION:\n",
    "    target_shape = tuple(encoder_output_shape[1:])\n",
    "    decoder = Dense(np.prod(target_shape)) (encoder_output)\n",
    "    decoder = Reshape(target_shape) (decoder)\n",
    "\n",
    "    decoder = Conv2DTranspose(filters = 64,\n",
    "                             kernel_size = (3, 3),\n",
    "                             strides = 2,\n",
    "                             padding = \"same\") (decoder)\n",
    "    decoder = LeakyReLU(negative_slope = negative_slope) (decoder)\n",
    "    decoder = BatchNormalization() (decoder)\n",
    "\n",
    "    decoder = Conv2DTranspose(filters = 32,\n",
    "                             kernel_size = (3, 3),\n",
    "                             strides = 2,\n",
    "                             padding = \"same\") (decoder)\n",
    "    decoder = LeakyReLU(negative_slope = negative_slope) (decoder)\n",
    "    decoder = BatchNormalization() (decoder)\n",
    "\n",
    "    decoder = Conv2DTranspose(filters = 1,\n",
    "                             kernel_size = (3, 3),\n",
    "                             padding = \"same\") (decoder)\n",
    "    outputs = Activation(activation = \"sigmoid\",\n",
    "                        name = \"decoder_output\") (decoder)\n",
    "\n",
    "\n",
    "    # FINALLY, BUILD THE AUTOENCODER AND RETURN IT:\n",
    "    autoencoder_model = Model(inputs, outputs)\n",
    "    return autoencoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a0001-036f-4356-95cc-672a2dd8f0c3",
   "metadata": {},
   "source": [
    "DEFINE A FUNCTION THAT WILL OUTPUT THE EUCLIDEAN DISTANCE BETWEEN TWO VECTORS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289c9916-67eb-4d87-9c0f-921b8ee20973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    return np.linalg.norm(x - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f40b62-0632-4abb-a375-bf9e08e72ef1",
   "metadata": {},
   "source": [
    "DEFINE THE SEARCH() FUNCTION, WHICH USES THE SEARCH INDEX(A DICTIONARY OF FEATURE VECTOR \n",
    "PAIRED WITH THEIR CORRESPONDING IMAGES) TO RETRIEVE THE MOST SIMILAR RESULTS\n",
    "TO A QUERY VECTOR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579b04d9-5399-40df-a944-3744fd6f8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_vector, search_index, max_results = 16):\n",
    "    vectors = search_index[\"features\"]\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(vectors)):\n",
    "        distance = euclidean_dist(query_vector,\n",
    "                                 vectors[i])\n",
    "\n",
    "        results.append((distance,\n",
    "                       search_index[\"images\"] [i]))\n",
    "        results = sorted(results,\n",
    "                        key = lambda p: p[0]) [:max_results]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99086087-0c32-4717-bd67-a9cedc32db01",
   "metadata": {},
   "source": [
    "LOAD THE FASHION-MNIST DATASET. KEEP ONLY THE IMAGES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8adb751d-efcd-405a-8492-961a7550ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da57739-6d4b-4175-b664-0dcf834db56e",
   "metadata": {},
   "source": [
    "NORMALIZE THE IMAGES AND ADD A COLOR CHANNEL DIMENSION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ee3005-f4cf-4bf0-a552-aece891998c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, axis = -1)\n",
    "x_test = np.expand_dims(x_test, axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d681268-4949-44ee-a927-36a418b92307",
   "metadata": {},
   "source": [
    "BUILD THE AUTOENCODER AND COMPILE IT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31351baf-0dd4-456e-9043-0f5ff70448fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = build_autoencoder()\n",
    "autoencoder.compile(optimizer = \"adam\", loss = \"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a54f0-28ad-4789-967a-d5ee70854771",
   "metadata": {},
   "source": [
    "TRAIN THE AUTOENCODER FOR 10 EPOCHS, ON BATCHES OF 512 IMAGES AT A TIME:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e9fa1f-7d4e-49d1-bc79-29e38313ccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 803ms/step - loss: 0.0393 - val_loss: 0.1294\n",
      "Epoch 2/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 775ms/step - loss: 0.0164 - val_loss: 0.0906\n",
      "Epoch 3/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 773ms/step - loss: 0.0125 - val_loss: 0.0526\n",
      "Epoch 4/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 778ms/step - loss: 0.0113 - val_loss: 0.0284\n",
      "Epoch 5/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 777ms/step - loss: 0.0106 - val_loss: 0.0145\n",
      "Epoch 6/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 784ms/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 7/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 774ms/step - loss: 0.0096 - val_loss: 0.0098\n",
      "Epoch 8/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 775ms/step - loss: 0.0094 - val_loss: 0.0096\n",
      "Epoch 9/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 770ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 10/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 780ms/step - loss: 0.0089 - val_loss: 0.0093\n",
      "Epoch 11/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 768ms/step - loss: 0.0087 - val_loss: 0.0088\n",
      "Epoch 12/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 771ms/step - loss: 0.0087 - val_loss: 0.0095\n",
      "Epoch 13/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 770ms/step - loss: 0.0085 - val_loss: 0.0088\n",
      "Epoch 14/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 769ms/step - loss: 0.0083 - val_loss: 0.0084\n",
      "Epoch 15/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 775ms/step - loss: 0.0083 - val_loss: 0.0096\n",
      "Epoch 16/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 769ms/step - loss: 0.0082 - val_loss: 0.0083\n",
      "Epoch 17/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 771ms/step - loss: 0.0080 - val_loss: 0.0091\n",
      "Epoch 18/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 768ms/step - loss: 0.0081 - val_loss: 0.0084\n",
      "Epoch 19/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 772ms/step - loss: 0.0080 - val_loss: 0.0087\n",
      "Epoch 20/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 776ms/step - loss: 0.0079 - val_loss: 0.0087\n",
      "Epoch 21/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 773ms/step - loss: 0.0078 - val_loss: 0.0079\n",
      "Epoch 22/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 768ms/step - loss: 0.0077 - val_loss: 0.0078\n",
      "Epoch 23/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 771ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 24/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 768ms/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 25/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 766ms/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 26/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 787ms/step - loss: 0.0075 - val_loss: 0.0082\n",
      "Epoch 27/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 777ms/step - loss: 0.0075 - val_loss: 0.0077\n",
      "Epoch 28/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 769ms/step - loss: 0.0075 - val_loss: 0.0084\n",
      "Epoch 29/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 772ms/step - loss: 0.0075 - val_loss: 0.0078\n",
      "Epoch 30/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 777ms/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 31/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 772ms/step - loss: 0.0073 - val_loss: 0.0079\n",
      "Epoch 32/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 772ms/step - loss: 0.0073 - val_loss: 0.0078\n",
      "Epoch 33/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 771ms/step - loss: 0.0073 - val_loss: 0.0077\n",
      "Epoch 34/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 774ms/step - loss: 0.0073 - val_loss: 0.0075\n",
      "Epoch 35/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 777ms/step - loss: 0.0073 - val_loss: 0.0075\n",
      "Epoch 36/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 772ms/step - loss: 0.0073 - val_loss: 0.0077\n",
      "Epoch 37/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 773ms/step - loss: 0.0072 - val_loss: 0.0075\n",
      "Epoch 38/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 773ms/step - loss: 0.0072 - val_loss: 0.0076\n",
      "Epoch 39/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 774ms/step - loss: 0.0072 - val_loss: 0.0076\n",
      "Epoch 40/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 777ms/step - loss: 0.0071 - val_loss: 0.0077\n",
      "Epoch 41/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 777ms/step - loss: 0.0071 - val_loss: 0.0085\n",
      "Epoch 42/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 771ms/step - loss: 0.0072 - val_loss: 0.0086\n",
      "Epoch 43/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 778ms/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 44/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 777ms/step - loss: 0.0070 - val_loss: 0.0075\n",
      "Epoch 45/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 774ms/step - loss: 0.0070 - val_loss: 0.0076\n",
      "Epoch 46/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 773ms/step - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 47/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 773ms/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 48/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 775ms/step - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 49/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 777ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 50/50\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 771ms/step - loss: 0.0070 - val_loss: 0.0072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1751e2c85d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 512\n",
    "autoencoder.fit(x_train, x_train,\n",
    "               epochs = EPOCHS,\n",
    "               batch_size = BATCH_SIZE,\n",
    "               shuffle = True,\n",
    "               validation_data = (x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6ebb3-b351-4471-9dd9-758e77ef6d3f",
   "metadata": {},
   "source": [
    "Create a new model, which we'll use as a feature extractor. It'll receive the same\n",
    "inputs as the autoencoder and will output the encoding learned by the autoencoder.\n",
    "In essence, we are using the encoder part of the autoencoder to turn images into\n",
    "vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b654dd-45c2-4b0f-b305-a6d96631e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_input = autoencoder.input\n",
    "fe_output = autoencoder.get_layer(\"encoder_output\").output\n",
    "feature_extractor = Model(inputs = fe_input,\n",
    "                         outputs = fe_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e2e865-7cd6-4589-899c-a81f02b1d8ab",
   "metadata": {},
   "source": [
    "Create the search index, comprised of the feature vectors of X_train, along with\n",
    "the original images (which must be reshaped back to 28x28 and rescaled to the\n",
    "range [0, 255]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "123ad38e-f120-48a8-835f-7051ad1a118e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "train_vectors = feature_extractor.predict(x_train)\n",
    "\n",
    "x_train = (x_train * 255.0).astype(\"uint8\")\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28))\n",
    "search_index = {\n",
    "    \"features\": train_vectors,\n",
    "    \"images\": x_train\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216d4d3d-0d4b-4508-bef5-20166505780d",
   "metadata": {},
   "source": [
    "Compute the feature vectors of X_test, which we will use as our sample of query\n",
    "images. Also, reshape X_test to 28x28 and rescale its values to the range [0, 255]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba15145a-2b5f-42d0-a72a-47eed84e01ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "test_vectors = feature_extractor.predict(x_test)\n",
    "\n",
    "x_test = (x_test * 255.0).astype(\"uint8\")\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008550be-200d-41bf-b703-55f665ddea41",
   "metadata": {},
   "source": [
    "Select 16 random test images (with their corresponding feature vectors) to use as\n",
    "queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e91a8e2-c8ef-4f29-9eea-aa8b77f7c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = np.random.randint(0, x_test.shape[0], 16)\n",
    "sample_images = x_test[sample_indices]\n",
    "sample_queries = test_vectors[sample_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd6ac61-8678-45f6-83a9-60612e6bd28d",
   "metadata": {},
   "source": [
    "Perform a search for each of the images in the test sample and save a side-to-side\n",
    "visual comparison of the test query, along with the results fetched from the index\n",
    "(which, remember, is comprised of the train data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8762c01-0c94-4234-b431-9fceed11a122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Only 1 search results found for 0, skipping.\n",
      "Warning: Only 1 search results found for 1, skipping.\n",
      "Warning: Only 1 search results found for 2, skipping.\n",
      "Warning: Only 1 search results found for 3, skipping.\n",
      "Warning: Only 1 search results found for 4, skipping.\n",
      "Warning: Only 1 search results found for 5, skipping.\n",
      "Warning: Only 1 search results found for 6, skipping.\n",
      "Warning: Only 1 search results found for 7, skipping.\n",
      "Warning: Only 1 search results found for 8, skipping.\n",
      "Warning: Only 1 search results found for 9, skipping.\n",
      "Warning: Only 1 search results found for 10, skipping.\n",
      "Warning: Only 1 search results found for 11, skipping.\n",
      "Warning: Only 1 search results found for 12, skipping.\n",
      "Warning: Only 1 search results found for 13, skipping.\n",
      "Warning: Only 1 search results found for 14, skipping.\n",
      "Warning: Only 1 search results found for 15, skipping.\n"
     ]
    }
   ],
   "source": [
    "for i, (vector, image) in enumerate(zip(sample_queries, sample_images)):\n",
    "    results = search(vector, search_index)\n",
    "    results = [r[1] for r in results]\n",
    "    if len(results) < 16:\n",
    "        print(f\"Warning: Only {len(results)} search results found for {i}, skipping.\")\n",
    "        continue  # Or break, depending on your workflow.\n",
    "\n",
    "    query_image = cv2.resize(image, (28 * 4, 28 * 4),\n",
    "                             interpolation=cv2.INTER_AREA)\n",
    "    results_mosaic = np.vstack([\n",
    "        np.hstack(results[0:4]),\n",
    "        np.hstack(results[4:8]),\n",
    "        np.hstack(results[8:12]),\n",
    "        np.hstack(results[12:16])\n",
    "    ])\n",
    "    result_image = np.hstack([query_image, results_mosaic])\n",
    "    cv2.imwrite(f\"{i}.jpg\", result_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a4088-bb3b-443d-b6ff-bbf3b0aeba4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
