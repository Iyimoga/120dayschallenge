{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890435f2-273d-492b-806d-66027b96dbda",
   "metadata": {},
   "source": [
    "# CREATING A CONVOLUTIONAL AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbe023e-1bf3-47fa-bdf9-6787966792fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bbd4c19-a182-4e7b-919e-e3d2deea75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303e846-3c0d-4811-853b-3768483edfb9",
   "metadata": {},
   "source": [
    "DEFINE THE BUILD_AYTOENCODER() FUNCTION, WHICH INTERNALLY BUILDS THE AQUTOENCODER ARCHITECTURE AND RETURNS THE ENCODER, THE DECODER, ADN THE AUTOENCODER ITSELF.\n",
    "    START DEFINING THE INPUT AND THE FIRST SET OF 32 CONVOLUTIONAL FILTERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7009b7ac-7247-4102-96b8-8905ca350054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_shape = (28, 28, 1),\n",
    "                     encoding_size = 32,\n",
    "                     alpha = 0.2):\n",
    "    inputs = Input(shape = input_shape)\n",
    "    encoder = Conv2D(filters = 32,\n",
    "                    kernel_size = (3, 3),\n",
    "                    strides = 2,\n",
    "                    padding = \"same\") (inputs)\n",
    "    encoder = LeakyReLU(alpha = alpha) (encoder)\n",
    "    encoder = BatchNormalization() (encoder)\n",
    "\n",
    "    # DEFINE THE SECOND SET OF CONVOLUTIONS(64 THIS TIME)\n",
    "\n",
    "    encoder = Conv2D(filters = 64,\n",
    "                    kernel_size = (3, 3),\n",
    "                    strides = 2,\n",
    "                    padding = \"same\") (encoder)\n",
    "    encoder = LeakyReLU(alpha = alpha) (encoder)\n",
    "    encoder = BatchNormalization() (encoder)\n",
    "\n",
    "    # DEFINE THE OUTPUT LAYERS OF THE ENCODER\n",
    "    encoder_output_shape = encoder.shape\n",
    "    encoder = Flatten() (encoder)\n",
    "    encoder_output = Dense(units = encoding_size) (encoder)\n",
    "\n",
    "    encoder_model = Model(inputs, encoder_output)\n",
    "\n",
    "#  IN STEP 2, WE DEFINED THE ENCODER MODEL, WHICH IS THE REGULAR CONVOLUTIONAL NEURAL NETWORK.\n",
    "# THE NEXT BLOCK DEFINES THE DECODER MODEL, STARTING WITH THE INPUT AND 64 TRANSPOSED CONVOLUTION FILTERS\n",
    "\n",
    "    decoder_input = Input(shape = (encoding_size,))\n",
    "    target_shape = tuple(encoder_output_shape[1:])\n",
    "    decoder = Dense(np.prod(target_shape)) (decoder_input)\n",
    "    decoder = Reshape(target_shape) (decoder)\n",
    "    decoder = Conv2DTranspose(filters = 64,\n",
    "                             kernel_size = (3, 3),\n",
    "                             strides = 2,\n",
    "                             padding = \"same\") (decoder)\n",
    "    decoder = LeakyReLU(alpha = alpha) (decoder)\n",
    "    decoder = BatchNormalization() (decoder)\n",
    "\n",
    "    # DEFINE THE SECOND SET OF TRANSPOSE (32 THIS TIME)\n",
    "\n",
    "    decoder = Conv2DTranspose(filters =32,\n",
    "                             kernel_size = (3, 2),\n",
    "                             strides = 2,\n",
    "                             padding = \"same\") (decoder)\n",
    "    decoder = LeakyReLU(alpha = alpha) (decoder)\n",
    "    decoder = BatchNormalization() (decoder)\n",
    "\n",
    "    # DEFINE THE INPUT LAYER OF THE DECODER\n",
    "    decoder = Conv2DTranspose(filters = 1,\n",
    "                             kernel_size = (3, 3),\n",
    "                             padding = \"same\") (decoder)\n",
    "    outputs = Activation(\"sigmoid\") (decoder)\n",
    "\n",
    "    decoder_model = Model(decoder_input, outputs)\n",
    "\n",
    "    # THE DECODER USES CONV2DTRANSPOSE LAYERS, WHICH EXPAND THEIR INPUTS TO GENERATE LAYER OUTPUT VOLUMES.\n",
    "    # NOTICE THAT FURTHER WE GO INTO THE DECODER, THE FEWER FILTERS THE CONV2DTRANSPOSE LAYERS USE. \n",
    "    # FINALLY, DEFINE THE AUTOENCODER:\n",
    "\n",
    "    encoder_model_output = encoder_model(inputs)\n",
    "    decoder_model_output = decoder_model(encoder_model_output)\n",
    "    autoencoder_model = Model(inputs, decoder_model_output)\n",
    "\n",
    "    return encoder_model, decoder_model, autoencoder_model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58523604-a544-4175-83ab-c0f3d946b0ac",
   "metadata": {},
   "source": [
    "The autoencoder is the end-to-end architecture. Tis starts with the input layer,\n",
    "which goes into the encoder, and ends with an output layer, which is the result of\n",
    "passing the encoder's output through the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ce793-4024-4dc0-b198-3bc4e8d06c07",
   "metadata": {},
   "source": [
    "Defne a function that will plot a sample of general images against their original\n",
    "counterparts. Tis will help us visually assess the autoencoder's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb4df875-7941-4f21-bdd7-3d1ca4d4c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original_vs_generated(original, generated):\n",
    "    num_images = 15\n",
    "    sample = np.random.randint(0, len(original),\n",
    "                              num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c3cf7-41e7-45bd-8b2c-23efa5d615fc",
   "metadata": {},
   "source": [
    "DEFINE AN INNER HELPER FUNCTION IN ORDER TO STACK A SAMPLE OF IMAGES IN A 3X3 GRID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fba3851f-2ab1-413a-8446-fc4a0030de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def stack(data):\n",
    "        images = data[sample]\n",
    "        return np.vstack([np.hstack(images[:5]),\n",
    "                         np.hstack(images[5:10]),\n",
    "                         np.hstack(images[10:15])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb6796-9b77-4ae1-bbf0-b78257f08a83",
   "metadata": {},
   "source": [
    "NEXT, DEFINE A FUNCTION THAT WILL PUT ON AN IMAGE IN A GIVEN POSITION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "672205ba-aafd-48bf-a7ef-8fdfb2251245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text(image, text, position):\n",
    "    pt1 = position\n",
    "    pt2 = (pt1[0] + 10 + (len(text) * 22), pt1[1] - 45)\n",
    "    cv2.rectangle(image, pt1, pt2, (255, 255, 255),\n",
    "                 -1)\n",
    "    cv2.putText(image, text,\n",
    "               position,\n",
    "               fontFace = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               fontScale = 1.3,\n",
    "               color = (0, 0, 0),\n",
    "               thickness = 4)\n",
    "\n",
    "    # FINALLY, CREATE A MOSAIC CONTAINING BOTH THE ORIGINAL AND THE GENERATED IMAGES:\n",
    "    original = stack(original)\n",
    "    generated = stack(generated)\n",
    "\n",
    "    mosaic = np.vsatck([original,\n",
    "                       generated])\n",
    "    mosaic = cv2.resize(mosaic, (860, 860),\n",
    "                       interpolation = cv2.INTER_AREA)\n",
    "    mosaic = cv2.cvOlor(mosaic, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    add_text(mosaic, \"original\", (50, 100))\n",
    "    add_text(mosaic, \"Generated\", (50, 520))\n",
    "\n",
    "    cv2.imshow(\"Mosaic\", mosaic)\n",
    "    cv2.waitKey(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d85bf-2a11-4f7c-a7c4-212cf20aed21",
   "metadata": {},
   "source": [
    "DOWNLOAD (OR LOAD, IF CACHED) FASHION_MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16d9b204-9304-4e4d-a6ef-bad232f1a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85552c0d-4695-493e-a504-3f3b2b8e6e81",
   "metadata": {},
   "source": [
    "NORMALIZE HE IMAGES AND ADD A CHANNEL DIMENSION TO THEM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f7aac3a-d7d1-42ef-b7a8-1ea579d2f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis = -1)\n",
    "x_test = np.expand_dims(x_test, axis = -1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02de6723-c556-4104-934f-645c4bff97cd",
   "metadata": {},
   "source": [
    "Here, we are only interested in the autoencoder, so we'll ignore the other two\n",
    "return values of the build_autoencoder() function. However, in diﬀerent\n",
    "circumstances, we could want to keep them. We'll train the model using 'adam'\n",
    "and use 'mse' as the loss function since we want to reduce the error, not optimize\n",
    "for classifcation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "566e21a3-22f1-4d41-a3cc-20293f4f5014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HomePC\\Desktop\\tensorflowProjects\\venv\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "_, _, autoencoder = build_autoencoder(encoding_size = 256)\n",
    "autoencoder.compile(optimizer = \"adam\", loss = \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7aca0b83-672c-4910-a5b5-0b688f2d9a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 695ms/step - loss: 0.0293 - val_loss: 0.1112\n",
      "Epoch 2/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 632ms/step - loss: 0.0087 - val_loss: 0.0936\n",
      "Epoch 3/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 631ms/step - loss: 0.0059 - val_loss: 0.0594\n",
      "Epoch 4/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 632ms/step - loss: 0.0047 - val_loss: 0.0252\n",
      "Epoch 5/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 660ms/step - loss: 0.0040 - val_loss: 0.0064\n",
      "Epoch 6/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 644ms/step - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 7/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 643ms/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 8/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 642ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 9/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 635ms/step - loss: 0.0027 - val_loss: 0.0033\n",
      "Epoch 10/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 655ms/step - loss: 0.0026 - val_loss: 0.0031\n",
      "Epoch 11/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 642ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 12/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 647ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 13/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 642ms/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 14/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 739ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 15/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 897ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 16/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 798ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 17/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 649ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 18/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 715ms/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 19/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 678ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 20/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 660ms/step - loss: 0.0020 - val_loss: 0.0023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fa33db9050>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "autoencoder.fit(x_train, x_train,\n",
    "               epochs = EPOCHS,\n",
    "               batch_size = BATCH_SIZE,\n",
    "               shuffle = True,\n",
    "               validation_data = (x_test, x_test),\n",
    "               verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5de580-77f4-4baa-bda3-d03cceb35632",
   "metadata": {},
   "source": [
    "MAKE COPIES OF THE TEST SET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21e171b9-55f7-4d04-b50c-b36ec90822d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3c282-59ee-4ec8-bb42-79de2005cb99",
   "metadata": {},
   "source": [
    "RESHAPE BOTH THE PREDICTIONS AND THE TEST IMAGES BACK TO 28X28(NO CHANNEL DIMENSION):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07e3a5fb-0fb4-4e23-8ac2-912d56840c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = (x_test.shape[0], 28, 28)\n",
    "predictions = predictions.reshape(original_shape)\n",
    "x_test = x_test.reshape(original_shape)\n",
    "predictions = (predictions * 255.0).astype(\"uint8\")\n",
    "x_test = (x_test * 255.0).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d097164-acc6-465a-9fb0-2649c8c04f2e",
   "metadata": {},
   "source": [
    "GENERATE A COMPARATIVE MOSAIC OF THE ORIGINAL IMAGES AND THE COPIES OUTPUTTED BY TH AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89089c2a-39f5-4417-9930-fef55d662e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_original_vs_generated(x_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45f070-4d3c-4b87-a446-486676ccd4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
