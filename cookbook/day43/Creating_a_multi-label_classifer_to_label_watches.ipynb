{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b0aca7-e879-459d-a043-ae6f443ae6d3",
   "metadata": {},
   "source": [
    "# Creating a multi-label classifer to label watches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4544f0ba-90a6-482c-a6e4-35cce4826f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d012d73-a64d-48a5-8ba4-dda8fd898f22",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce18e248-0867-4d40-a589-f2dffc57a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from csv import DictReader\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3bf5a-82ca-4446-932a-fbf8589369de",
   "metadata": {},
   "source": [
    "## Define a function to build the network architecture. First, implement the convolutional blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fd6801a-4dee-4b45-a7aa-b31a2a5ac35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(width, height, depth, classes):\n",
    "    input_layer = Input(shape = (width, height, depth))\n",
    "    x = Conv2D(filters = 32,\n",
    "              kernel_size = (3, 3),\n",
    "              padding = \"same\") (input_layer)\n",
    "\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis = -1) (x)\n",
    "    x = Conv2D(filters = 32,\n",
    "              kernel_size = (3, 3),\n",
    "              padding = \"same\") (x)\n",
    "    x = ReLU() (x)\n",
    "    x = BatchNormalization(axis = -1) (x)\n",
    "    x = MaxPooling2D(pool_size = (2, 2)) (x)\n",
    "    x = Dropout(rate = 0.25) (x)\n",
    "\n",
    "    x = Conv2D(filters = 64,\n",
    "              kernel_size = (3, 3),\n",
    "              padding = \"same\") (x)\n",
    "    x = ReLU() (x)\n",
    "    x = BatchNormalization(axis = -1) (x)\n",
    "    x = Conv2D(filters = 64,\n",
    "              kernel_size = (3, 3),\n",
    "              padding = \"same\") (x)\n",
    "\n",
    "    x = ReLU() (x)\n",
    "    x = BatchNormalization(axis = -1) (x)\n",
    "    x = MaxPooling2D(pool_size = (2, 2)) (x)\n",
    "    x = Dropout(rate = 0.25) (x)\n",
    "    \n",
    "#Next, add the convolutional layers\n",
    "    x = Flatten() (x)\n",
    "    x = Dense(units = 512) (x)\n",
    "    x = ReLU() (x)\n",
    "    x = BatchNormalization(axis = -1) (x)\n",
    "    x = Dropout(rate = 0.5) (x)\n",
    "    \n",
    "    x = Dense(units = classes) (x)\n",
    "    output  = Activation(\"sigmoid\") (x)\n",
    "\n",
    "    return Model(input_layer, output)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638b88b-132a-4bd0-9980-1af141a87196",
   "metadata": {},
   "source": [
    "## Define function to load all images and labels(gender and usage), given a list of image paths and a dictionary of metadata associated with each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2f590b7-be5c-4898-8b24-c721053b5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(image_paths, styles,\n",
    "                          target_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image_path in image_paths:\n",
    "        image = load_img(image_path,\n",
    "                        target_size = target_size)\n",
    "        image = img_to_array(image)\n",
    "        image_id = image_path.split(os.path.sep) [-1] [:-4]\n",
    "\n",
    "        image_style = styles[image_id]\n",
    "        label = (image_style[\"gender\"],\n",
    "                image_style[\"usage\"])\n",
    "\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972ca81-1684-4a4f-b870-f5d0e12691b7",
   "metadata": {},
   "source": [
    "## Set the random seed to guarantee reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2974dfd7-0ffc-4a27-b498-eb62a224a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 999\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b742c-ad68-4847-96ea-096c46f74f07",
   "metadata": {},
   "source": [
    "## Define the path to the images and the styles.csv metadata file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78815e72-577c-425c-ba57-c5c62973d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = (pathlib.Path.home() / \"C:/.keras\" /\n",
    "            \"datasets\" /\n",
    "            \"fashion-product-images-small\")\n",
    "styles_path = str(base_path / \"styles.csv\")\n",
    "images_path_pattern = str(base_path / \"images/*.jpg\")\n",
    "image_paths = glob.glob(images_path_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac04ec-ce10-42b2-9950-fdb03b9cbea8",
   "metadata": {},
   "source": [
    "## Keep only the watches images for Casual, Smart Casual, and Formal usage, suited to Men and Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dc8a77a-fe93-4d60-a5db-35c7568de5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(styles_path, \"r\") as f:\n",
    "    dict_reader = DictReader(f)\n",
    "    STYLES = [*dict_reader]\n",
    "\n",
    "    article_type = \"Watches\"\n",
    "    genders = {\"Men\", \"Women\"}\n",
    "    usages = {\"Casual\", \"Smart Casual\", \"Formal\"}\n",
    "    STYLES = {style[\"id\"]: style\n",
    "             for style in STYLES\n",
    "             if (style[\"articleType\"] == article_type\n",
    "                and\n",
    "                style[\"gender\"] in genders and \n",
    "                style[\"usage\"] in usages)}\n",
    "\n",
    "    image_paths = [*filter(lambda p:\n",
    "                          p.split(os.path.sep) [-1] [:-4]\n",
    "                          in STYLES.keys(),\n",
    "                          image_paths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ba44d-2bf3-4d1c-bf98-d4a31db59a4b",
   "metadata": {},
   "source": [
    "## Load images and labels, resizing  the images into a 64x64x3 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45268ab3-7f8a-4cbf-bcd0-575adbc1eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_images_and_labels(image_paths, STYLES,\n",
    "                             (64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861bfdf7-64ca-436d-b28e-906ecec782ab",
   "metadata": {},
   "source": [
    "## Normalize the images and mult-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52b577c9-a793-4ea1-9dd7-9310e2b57a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.astype(\"float\") / 255.0\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951994a4-6715-4f60-aa06-8986ad42cfca",
   "metadata": {},
   "source": [
    "## Create the train, validation, and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6035142b-68c8-481a-8c56-7326922619d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_test, y_train, y_test) = train_test_split(x, y,\n",
    "                                                     stratify = y,\n",
    "                                                     test_size = 0.2,\n",
    "                                                     random_state = SEED)\n",
    "\n",
    "(x_train, x_valid, y_train, y_valid) = train_test_split(x_train, y_train,\n",
    "                                                       stratify = y_train,\n",
    "                                                       test_size = 0.2,\n",
    "                                                       random_state = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b629be7-4bf5-4690-8ab2-fb17b89b9cdd",
   "metadata": {},
   "source": [
    "## Build and Compile the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c58269b-6d66-4e61-ae2b-e306456a3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_network(width = 64,\n",
    "                     height = 64,\n",
    "                     depth = 3,\n",
    "                     classes = len(mlb.classes_))\n",
    "model.compile(loss = \"binary_crossentropy\",\n",
    "             optimizer = \"rmsprop\",\n",
    "             metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96935e-7333-4ef2-bc85-43ad5d465f44",
   "metadata": {},
   "source": [
    "## Train the Model for 20 epochs, in batches of 64 images at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a9ac643-b132-422b-b1c9-430add53a166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.2597 - loss: 0.4841 - val_accuracy: 0.9284 - val_loss: 0.6328\n",
      "Epoch 2/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.3877 - loss: 0.3479 - val_accuracy: 0.9761 - val_loss: 0.7172\n",
      "Epoch 3/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.4988 - loss: 0.2267 - val_accuracy: 0.9814 - val_loss: 0.6573\n",
      "Epoch 4/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.6375 - loss: 0.1824 - val_accuracy: 0.9814 - val_loss: 0.6149\n",
      "Epoch 5/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.6546 - loss: 0.1366 - val_accuracy: 0.9814 - val_loss: 0.6432\n",
      "Epoch 6/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.7298 - loss: 0.1348 - val_accuracy: 0.9814 - val_loss: 0.8243\n",
      "Epoch 7/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.7173 - loss: 0.1015 - val_accuracy: 0.9814 - val_loss: 1.0478\n",
      "Epoch 8/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.6937 - loss: 0.0936 - val_accuracy: 0.9814 - val_loss: 0.6233\n",
      "Epoch 9/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.6706 - loss: 0.0605 - val_accuracy: 0.9788 - val_loss: 1.1505\n",
      "Epoch 10/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.7107 - loss: 0.0689 - val_accuracy: 0.9814 - val_loss: 0.8675\n",
      "Epoch 11/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.7340 - loss: 0.0688 - val_accuracy: 0.9814 - val_loss: 0.6221\n",
      "Epoch 12/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.7417 - loss: 0.0532 - val_accuracy: 0.9814 - val_loss: 0.4769\n",
      "Epoch 13/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.7068 - loss: 0.0501 - val_accuracy: 0.9814 - val_loss: 0.4315\n",
      "Epoch 14/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.7272 - loss: 0.0395 - val_accuracy: 0.9496 - val_loss: 1.1329\n",
      "Epoch 15/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.6961 - loss: 0.0338 - val_accuracy: 0.9576 - val_loss: 0.5614\n",
      "Epoch 16/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.6883 - loss: 0.0362 - val_accuracy: 0.7692 - val_loss: 0.7410\n",
      "Epoch 17/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.6393 - loss: 0.0270 - val_accuracy: 0.9257 - val_loss: 0.2584\n",
      "Epoch 18/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.6598 - loss: 0.0300 - val_accuracy: 0.7878 - val_loss: 0.4032\n",
      "Epoch 19/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.6409 - loss: 0.0322 - val_accuracy: 0.9231 - val_loss: 0.2774\n",
      "Epoch 20/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.6785 - loss: 0.0305 - val_accuracy: 0.7719 - val_loss: 0.2716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1617f11b0d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "model.fit(x_train, y_train,\n",
    "         validation_data = (x_valid, y_valid),\n",
    "         batch_size = BATCH_SIZE,\n",
    "         epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a607eafb-1b49-4164-975c-6a325b910eab",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d44565e-3892-4ab5-9f9b-dd2951636906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 215ms/step - accuracy: 0.7575 - loss: 0.2680\n",
      "Test accuracy: 0.781316339969635\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test, y_test,\n",
    "                       batch_size = BATCH_SIZE)\n",
    "print(f\"Test accuracy: {result[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362381d5-2819-48ec-af79-ee4a1f5a6618",
   "metadata": {},
   "source": [
    "## Use the model to make predictions on a test image, displaying the probability of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db14ea85-81d2-47a6-b83c-b9ced99973e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 523ms/step\n",
      "Casual: 99.87%\n",
      "Formal: 0.11%\n",
      "Men: 19.03%\n",
      "Smart Casual: 0.25%\n",
      "Women: 80.86%\n"
     ]
    }
   ],
   "source": [
    "test_image = np.expand_dims(x_test[0], axis = 0)\n",
    "probabilities = model.predict(test_image) [0]\n",
    "for label, p in zip(mlb.classes_, probabilities):\n",
    "    print(f\"{label}: {p * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962a57bf-f3f5-442d-8220-b059b813d0b5",
   "metadata": {},
   "source": [
    "## Compare the ground truth labels with the network's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23d903e7-82ef-40f6-8677-b67239d7b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groung truth labels: [(np.str_('Casual'), np.str_('Women'))]\n"
     ]
    }
   ],
   "source": [
    "ground_truth_labels = np.expand_dims(y_test[0],\n",
    "                                    axis = 0)\n",
    "ground_truth_labels = mlb.inverse_transform(ground_truth_labels)\n",
    "print(f\"Groung truth labels: {ground_truth_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9768b25-9c86-4951-afc8-dd76a0dceb29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
