{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SAVING AND LOADING A MODEL**"
      ],
      "metadata": {
        "id": "P-OADXjQOPy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a neural network is hard work and time-consuming. Tat's why retraining a\n",
        "model every time is impractical. Te good news is that we can save a network to disk and\n",
        "load it whenever we need it, whether to improve its performance with more training or to\n",
        "use it to make predictions on fresh data. In this recipe, we'll learn about diï¬€erent ways to\n",
        "persist a model."
      ],
      "metadata": {
        "id": "i3YhF_0tOfji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTING LIBRARIES"
      ],
      "metadata": {
        "id": "MIip38dUOjaN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ogTYnew5OL7O"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import ReLU\n",
        "from tensorflow.keras.layers import Softmax\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defne a function that will download and prepare the data by normalizing the train and test sets and one-hot encoding the labels"
      ],
      "metadata": {
        "id": "zJAYTW1ASEiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "  # NORMALIZE DATA\n",
        "  x_train  = x_train.astype(\"float32\") / 255.0\n",
        "  x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "  # reshape grayscale to include channel dimension\n",
        "  x_train = np.expand_dims(x_train, axis=3)\n",
        "  x_test = np.expand_dims(x_test, axis=3)\n",
        "\n",
        "  # Process labels\n",
        "  label_binarizer = LabelBinarizer()\n",
        "  y_train = label_binarizer.fit_transform(y_train)\n",
        "  y_test = label_binarizer.transform(y_test)\n",
        "\n",
        "  return x_train, y_train, x_test, y_test\n"
      ],
      "metadata": {
        "id": "aZaJVv19Rgwf"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defne a function for building a network. Te architecture comprises a single convolutional layer and two fully connected layers:"
      ],
      "metadata": {
        "id": "f5tCYNSYZsKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_network():\n",
        "  input_layer = Input(shape = (28, 28, 1))\n",
        "  convolution_1 = Conv2D(kernel_size = (2, 2),\n",
        "                         padding = \"same\",\n",
        "                         strides = (2, 2),\n",
        "                         filters = 32) (input_layer)\n",
        "  activation_1 = ReLU() (convolution_1)\n",
        "  batch_normalization_1 = BatchNormalization() (activation_1)\n",
        "  pooling_1 = MaxPooling2D(pool_size = (2, 2), strides = (1, 1), padding = \"same\") (batch_normalization_1)\n",
        "  dropout = Dropout(rate = 0.5) (pooling_1)\n",
        "\n",
        "  flatten  = Flatten() (dropout)\n",
        "  dense_1 = Dense(units = 128) (flatten)\n",
        "  activation_2 = ReLU() (dense_1)\n",
        "  dense_2 = Dense(units = 10) (activation_2)\n",
        "  output = Softmax() (dense_2)\n",
        "\n",
        "  network = Model(inputs = input_layer, outputs = output)\n",
        "  return network"
      ],
      "metadata": {
        "id": "bwAUGGfQZn99"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement a function that will evaluate a network using the test set:"
      ],
      "metadata": {
        "id": "hTiIrkYrdEdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, x_test, y_test):\n",
        "  _, accuracy = model.evaluate(x_test, y_test, verbose = 0)\n",
        "  print(f\"accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "GGD0rPplc1kf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cX43hO38dgvq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the data, create a validation split, and instantiate the neural network:"
      ],
      "metadata": {
        "id": "l6PmQd23dkRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = load_data()\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8)\n",
        "model = build_network()"
      ],
      "metadata": {
        "id": "VzMUqJFfdlL9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile and train the model for 50 epochs, with a batch size of 1024. Feel free to tune these values according to the capacity of your machine:"
      ],
      "metadata": {
        "id": "dOnrrk5ji0Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=50, batch_size=1024, verbose=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6-gxNeJeKPq",
        "outputId": "223ef397-e20f-41a7-a846-76099f6c2275"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a766b167e00>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model, along with its weights, in HDF5 format using the save() method. Ten, load the persisted model using load_model() and evaluate the network's performance on the test set:"
      ],
      "metadata": {
        "id": "4K_tIbxvjMHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model and weights as HDF5.\n",
        "model.save('model_and_weights.hdf5')\n",
        "\n",
        "# Loading model and weights as HDF5.\n",
        "loaded_model = load_model('model_and_weights.hdf5')\n",
        "\n",
        "# Predicting using loaded model.\n",
        "evaluate(loaded_model, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6cYQbfojCuO",
        "outputId": "b43f2729-8f84-4d18-c8c4-fc3dc84272f1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.9840999841690063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a6ju4NaauApf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}