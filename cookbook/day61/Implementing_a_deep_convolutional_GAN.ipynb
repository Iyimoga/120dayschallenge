{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b0aa8c-b6c9-4c23-bda9-db5e20f522a1",
   "metadata": {},
   "source": [
    "# Implementing a deep convolutional GAN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "144111dd-5926-4699-8a85-5a7cfd721550",
   "metadata": {},
   "source": [
    "Let study one of the most captivating and promising types of neural\n",
    "networks: Generative Adversarial Networks (GANs). As the term implies, these\n",
    "networks are actually a system comprised of two sub-networks: the generator and the\n",
    "discriminator. The job of the generator is to produce images so good that they could come\n",
    "from the original distribution (but actually don't; they're generated from scratch), thereby\n",
    "fooling the discriminator, whose task is to discern between real and fake images.\n",
    "    \n",
    "GANs are the tip of the spear in areas such as semi-supervised learning and image-to-image translation, \n",
    "both topics that we will cover in this chapter. As a complement, the\n",
    "final recipe in this chapter teaches us how to perform an adversarial attack on a network\n",
    "using the Fast Gradient Signed Method (FGSM)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "651c7658-b01c-4b70-89f6-fe660a709fa3",
   "metadata": {},
   "source": [
    "A GAN is comprised, in its simplest form, of two networks, a generator and a\n",
    "discriminator. The discriminator is just a regular Convolutional Neural Network (CNN)\n",
    "that must solve the binary classifcation problem of distinguishing real images from fakes.\n",
    "The generator, on the other hand, is similar to the decoder in an autoencoder because it\n",
    "has to produce an image from a seed, which is just a vector of Gaussian noise.\n",
    "In this recipe, we'll implement a Deep Convolutional Generative Adversarial Network\n",
    "(DCGAN) to produce images akin to the ones present in EMNIST, a dataset that extends\n",
    "the well-known MNIST dataset with uppercase and lowercase handwritten letters on top\n",
    "of the digits from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3ae5e0-19b3-49a9-9ff6-2b98c4e8395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d83024-91b3-4cc9-b329-c86ee7c4a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO LOCATION OF AN INSTALLED PACKAGE\n",
    "\n",
    "#!pip show tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d3a063-8d5b-435d-92cc-ada0d2a61804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO CHECK PACKAGE ENVIRONMENT\n",
    "\n",
    "#import sys\n",
    "#print(sys.executable)\n",
    "#import tensorflow_datasets as tfds\n",
    "#print(tfds.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b66b081b-eeb3-4463-97d6-d8164fecfbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0891e1b-1541-42f8-8786-261cc6ef47ed",
   "metadata": {},
   "source": [
    "DEFINE AN ALIAS FOR THE AUTOTUNE SETTING, WHICH WE'LL USE LATER\n",
    "TO DETERMINE THE NUMBER OF PARALLEL CALLS WHEN PROCESSING THE IMAGES\n",
    "IN THE DATASET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03988191-22b7-4f1c-af4b-fc13a6086e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f9739-9a95-45d3-b0b0-7097bcad5645",
   "metadata": {},
   "source": [
    "DEFINE A DCGAN() CLASS TO ENCAPSULATE OUR IMPLEMENTATION.\n",
    "THE CONSTRUCTOR CREATE THE DISCRIMINATOR, GENERATOR, LOSS FUNCTION,\n",
    "AND THE RSPECTIVE OPTIMIZERS FOR BOTH SUB-NETWORKS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "96195661-3f86-4096-a0ea-ad3dcb090d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.loss = BinaryCrossentropy(from_logits = True)\n",
    "        self.generator = self.create_generator()\n",
    "        self.discriminator = self.create_discriminator()\n",
    "        self.generator_opt = Adam(learning_rate = 1e-4)\n",
    "        self.discriminator_opt = Adam(learning_rate = 1e-4)\n",
    "\n",
    "        '''\n",
    "        Defne a static method to create the generator network. It reconstructs a 28x28x1\n",
    "        image from an input tensor of 100 elements. Notice the use of transposed\n",
    "        convolutions (Conv2DTranspose) to expand the output volumes as we go deeper\n",
    "        into the network. Also, notice the activation is 'tanh', which means the outputs\n",
    "        will be in the range [-1, 1]:\n",
    "        '''\n",
    "    @staticmethod\n",
    "    def create_generator(negative_slope = 0.3):\n",
    "        input = Input(shape = (100,))\n",
    "        x = Dense(units = 7 * 7 *256,\n",
    "                 use_bias = False) (input)\n",
    "        x = LeakyReLU(negative_slope = negative_slope) (x)\n",
    "        x = BatchNormalization() (x)\n",
    "\n",
    "        x = Reshape((7, 7, 256)) (x)\n",
    "\n",
    "        # Add the frst transposed convolution block, with 128 flters:\n",
    "        x = Conv2DTranspose(filters = 128,\n",
    "                           strides = (1, 1),\n",
    "                           kernel_size = (5, 5),\n",
    "                           padding = \"same\",\n",
    "                           use_bias = False) (x)\n",
    "        x = LeakyReLU(negative_slope = negative_slope) (x)\n",
    "        x = BatchNormalization() (x)\n",
    "\n",
    "        # Create the second transposed convolution block, with 64 flters:\n",
    "        x = Conv2DTranspose(filters = 64,\n",
    "                           strides = (2, 2),\n",
    "                           kernel_size = (5, 5),\n",
    "                           padding = \"same\",\n",
    "                           use_bias = False) (x)\n",
    "        x = LeakyReLU(negative_slope = negative_slope) (x)\n",
    "        x = BatchNormalization() (x)\n",
    "\n",
    "        # add the last transposed convolution block, with only one filter, corresponding to\n",
    "        #the output of the network:\n",
    "        x = Conv2DTranspose(filters = 1,\n",
    "                           strides = (2, 2),\n",
    "                           kernel_size = (5, 5),\n",
    "                           padding = \"same\",\n",
    "                           use_bias = False) (x)\n",
    "        output = Activation(\"tanh\") (x)\n",
    "\n",
    "        return Model(input, output)\n",
    "# Defne a static method to create the discriminator. \n",
    "# This architecture is a regular CNN:\n",
    "    @staticmethod\n",
    "    def create_discriminator(negative_slope = 0.3, dropout = 0.3):\n",
    "        input = Input(shape = (28, 28, 1))\n",
    "        x = Conv2D(filters = 64, \n",
    "                  kernel_size = (5, 5),\n",
    "                  strides = (2, 2),\n",
    "                  padding = \"same\") (input)\n",
    "        x = LeakyReLU(negative_slope = negative_slope) (x)\n",
    "        x = Dropout(rate = dropout) (x)\n",
    "\n",
    "        x = Conv2D(filters = 128,\n",
    "                  kernel_size = (5, 5),\n",
    "                  strides = (2, 2),\n",
    "                  padding =\"same\") (x)\n",
    "        x = LeakyReLU(negative_slope = negative_slope) (x)\n",
    "        x = Dropout(rate = dropout) (x)\n",
    "\n",
    "        x = Flatten() (x)\n",
    "        output = Dense(units = 1) (x)\n",
    "\n",
    "        return Model(input, output)\n",
    "    # Defne a method to calculate the discriminator's loss, \n",
    "    # which is the sum of the real and fake losses:\n",
    "    def discriminator_loss(self, real, fake):\n",
    "        real_loss = self.loss(tf.ones_like(real), real)\n",
    "        fake_loss = self.loss(tf.zeros_like(fake), fake)\n",
    "\n",
    "        return real_loss + fake_loss\n",
    "\n",
    "    # Defne a method to calculate the generator's loss:\n",
    "    def generator_loss(self, fake):\n",
    "        return self.loss(tf.ones_like(fake), fake)\n",
    "\n",
    "        # Defne a method to perform a single training step. \n",
    "        # We'll start by generating a vector of random Gaussian noise:\n",
    "    @tf.function\n",
    "    def train_step(self, images, batch_size):\n",
    "        noise = tf.random.normal((batch_size, noise_dimension))\n",
    "\n",
    "        # pass the random noise to the generator to produce fake images:\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "            generated_images = self.generator(noise,\n",
    "                                             training = True)\n",
    "\n",
    "            # Pass the real and fake images to the discriminator \n",
    "            # and compute the losses of both sub-networks:\n",
    "            real = self.discriminator(images, \n",
    "                                     training = True)\n",
    "            fake = self.discriminator(generated_images,\n",
    "                                     training = True)\n",
    "\n",
    "            gen_loss = self.generator_loss(fake)\n",
    "            disc_loss = self.discriminator_loss(real,\n",
    "                                               fake)\n",
    "        # Compute the gradients:\n",
    "        generator_grad = gen_tape.gradient(gen_loss,\n",
    "                 self.generator.trainable_variables)\n",
    "        discriminator_grad = dis_tape.gradient(disc_loss,\n",
    "                                                   self.discriminator.trainable_variables)\n",
    "\n",
    "        # Apply the gradients using the respective optimizers:\n",
    "        opt_args = zip(generator_grad,\n",
    "                      self.generator.trainable_variables)\n",
    "        self.generator_opt.apply_gradients(opt_args)\n",
    "\n",
    "        opt_args = zip(discriminator_grad,\n",
    "                      self.discriminator.trainable_variables)\n",
    "        self.discriminator_opt.apply_gradients(opt_args)\n",
    "\n",
    "    # defne a method to train the whole architecture. Every 10 epochs,\n",
    "    # we will plot the images the generator produces in order to \n",
    "    # visually assess their quality:\n",
    "    def train(self, dataset, test_seed, epochs, batch_size):\n",
    "                     for epoch in tqdm(range(epochs)):\n",
    "                        for image_batch in dataset:\n",
    "                            self.train_step(image_batch,\n",
    "                                           batch_size)\n",
    "                        if epoch == 0 or epoch % 10 == 0:\n",
    "                            generate_and_save_images(self.generator,\n",
    "                                                    epoch,\n",
    "                                                    test_seed)\n",
    "                                                    \n",
    "\n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73522c98-7cdb-4ce6-82c0-45341c8af519",
   "metadata": {},
   "source": [
    "Defne a function to produce new images, and then save a 4x4 mosaic of them to\n",
    "disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae7b275d-a47e-4258-99e3-3cbaf3342afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training = False)\n",
    "\n",
    "    plt.figure(figsize = (4, 4))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        image = predictions[i, :, :, 0] * 127.5 + 127.5\n",
    "        image = tf.cast(image, tf.uint8)\n",
    "        plt.imshow(image, cmap = \"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.savefig(f\"{epoch}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6692f7-76ea-4996-b14e-d21c1ab49b16",
   "metadata": {},
   "source": [
    "DEFINE A FUNCTION TO SCALE THE IMAGES THAT COME FROM THE EMNIST\n",
    "DATASET TO THE [-1, 1] INTERVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a5066d82-e0b3-4c2e-875a-e9bc604798a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(input):\n",
    "    image = tf.cast(input[\"image\"], tf.float32)\n",
    "    image = (image - 127.5) / 127.5\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23f047-42be-4f17-ae07-1d6be6d93fa3",
   "metadata": {},
   "source": [
    "LOAD THE EMNIST DATASET USING TFDS. WE'LL USE ONLY THE \"TRAIN\" SPLIT\n",
    "WHICH CONTAINS MORE THAN 600,000 IMAGES.\n",
    "WE'LL ALSO MAKE SURE TO SCALE EACH IMAGE USING THE \"TANH\" RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d90b5736-ee1c-4067-a80e-026e713caf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 512\n",
    "train_dataset = (tfds.load(\"emnist\", split = \"train\")\n",
    "                .map(process_image,\n",
    "                    num_parallel_calls = AUTOTUNE)\n",
    "                .shuffle(BUFFER_SIZE)\n",
    "                .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb52fa-3863-415e-87aa-4b41a8c8bf81",
   "metadata": {},
   "source": [
    "CREATE A TEST SEED THAT WILL BE USED THROUGHOUT THE TRAINING\n",
    "OF THE DCGAN TO GENERATE IMAGES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "daaeda20-261c-4dd9-9998-f7738ef6deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dimension = 100\n",
    "num_examples_to_generate = 16\n",
    "seed_shape = (num_examples_to_generate,\n",
    "             noise_dimension)\n",
    "test_seed = tf.random.normal(seed_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db6162-d93b-483b-a8b2-ef27d5050436",
   "metadata": {},
   "source": [
    "INSTANTIATE AND TRAIN A DCGAN() INSTANCE FOR 100 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "51ab25d4-a253-46b3-b4a5-8ebd040aaff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "dcgan = DCGAN()\n",
    "dcgan.train(train_dataset, test_seed, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf332ea-77da-435c-af03-8e7ace1370ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
