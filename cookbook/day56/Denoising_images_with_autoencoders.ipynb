{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c0e6a0-3997-4f3c-9cca-92b2c42023cb",
   "metadata": {},
   "source": [
    "# DENOISING IMAGES WITH AUTOENCODERS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29b25b75-cbc4-44e9-90ae-f072ba764e09",
   "metadata": {},
   "source": [
    "sing images to reconstruct their input is great, but are there more useful ways to apply\n",
    "autoencoders? Of course there are! One of them is image denoising. As the name suggests,\n",
    "this is the act of restoring damaged images by replacing the corrupted pixels and regions\n",
    "with sensible values.\n",
    "\n",
    "In this recipe, we'll purposely damage the images in Fashion-MNIST, and then train an\n",
    "autoencoder to denoise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae4b400-893b-4597-a6c6-ca13615e8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab23d3d-e0f2-4f1f-aae2-5dd6ff5d2746",
   "metadata": {},
   "source": [
    "DEFINE THE BUILD_AUTOENCODER() FUNCTION, WHICH CREATES THE CORRESPONDING NEURAL ARCHITECTURE.\n",
    "    NOTICE THAT THIS IS THE SAME ARCHITECTURE WE IMPLEMENTED I THE PREVIOUS RECIPE; THEREFORE, WE WON'T \n",
    "GO INTO TOO MUCH DETAIL HERE. FOR AN IN-DEPTH EXPLANATION, PLEASE REFER TO THE \"CREATING AUTOENCODER\" RECIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8d043d-f789-4a08-bd63-302418f7ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_shape = (28, 28, 1),\n",
    "                     encoding_size = 128,\n",
    "                     alpha = 0.2):\n",
    "    inputs = Input(shape = input_shape)\n",
    "    encoder = Conv2D(filters = 32,\n",
    "                    kernel_size = (3, 3),\n",
    "                    strides = 2,\n",
    "                    padding = \"same\") (inputs)\n",
    "    encoder = LeakyReLU(negative_slope = 0.3) (encoder)\n",
    "    encoder = BatchNormalization() (encoder)\n",
    "    encoder = Conv2D(filters = 64,\n",
    "                    kernel_size = (3, 3),\n",
    "                    strides = 2,\n",
    "                    padding = \"same\") (encoder)\n",
    "    encoder = LeakyReLU(negative_slope = 0.3) (encoder)\n",
    "    encoder = BatchNormalization() (encoder)\n",
    "\n",
    "    encoder_output_shape = encoder.shape\n",
    "    encoder = Flatten() (encoder)\n",
    "    encoder_output = Dense(units = encoding_size) (encoder)\n",
    "\n",
    "    encoder_model = Model(inputs, encoder_output)\n",
    "\n",
    "\n",
    "    # LET'S CREATE THE DECODER\n",
    "    decoder_input = Input(shape = (encoding_size, ))\n",
    "    target_shape = tuple(encoder_output_shape[1:])\n",
    "    decoder = Dense(np.prod(target_shape)) (decoder_input)\n",
    "\n",
    "    decoder = Reshape(target_shape) (decoder)\n",
    "\n",
    "    decoder = Conv2DTranspose(filters = 64,\n",
    "                             kernel_size = (3, 3),\n",
    "                             strides = 2,\n",
    "                             padding = \"same\") (decoder)\n",
    "    decoder = LeakyReLU(negative_slope = 0.3) (decoder)\n",
    "    decoder = BatchNormalization() (decoder)\n",
    "    \n",
    "    decoder = Conv2DTranspose(filters = 32,\n",
    "                             kernel_size = (3, 3),\n",
    "                             strides = 2,\n",
    "                             padding = \"same\") (decoder)\n",
    "    decoder = LeakyReLU(negative_slope = 0.3) (decoder)\n",
    "    decoder = BatchNormalization() (decoder)\n",
    "    \n",
    "    decoder = Conv2DTranspose(filters = 1,\n",
    "                             kernel_size = (3, 3),\n",
    "                             padding = \"same\") (decoder)\n",
    "    outputs = Activation(\"sigmoid\") (decoder)\n",
    "    \n",
    "    decoder_model = Model(decoder_input, outputs)\n",
    "\n",
    "    # DEFINE THE ENCODER ITSELF\n",
    "    encoder_model_output = encoder_model(inputs)\n",
    "    decoder_model_output = decoder_model(encoder_model_output)\n",
    "    autoencoder_model = Model(inputs,\n",
    "                             decoder_model_output)\n",
    "\n",
    "    return encoder_model, decoder_model, autoencoder_model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c7c0a-dc7a-47f7-a2d6-acceb691158e",
   "metadata": {},
   "source": [
    "DEFINE THE PLOT_ORIGINAL_VS_GENERATED() FUNCTION, WHICH CREATES A COMPARATIVE MOSAIC OF THE ORIGINAL AND GENERATED\n",
    "IMAGES.\n",
    "WE WILL USE THIS FUNCTION LATER TO SHOW THE NOISY IMAGES AND THEIR RESTORED COUNTERPARTS.\n",
    "SIMILAR TO BUILD_AUTOENCODER(), THIS FUNCTION WORKS IN THE SAME WAY WE DEFINED IT IN THE \"CREATING A SIMPLE FULLY CONNECTED AUTOENCODER\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "154b4ef8-9f5e-4036-95a1-90241b543939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original_vs_generated(original, generated):\n",
    "    num_images = 15\n",
    "    sample = np.random.randint(0, len(original),\n",
    "                              num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57321d96-57ca-4b67-9aa1-808aa1d2461e",
   "metadata": {},
   "source": [
    "DEFINE AN INNER HELPER FUNCTION THAT WILL STACK A SAMPLE OF IMAGES IN A 3X5 GRID:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ff9f06-c79c-4035-86d7-95e5af7d4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def stack(data):\n",
    "        images = data[sample]\n",
    "        return np.vstack([np.hstack(images[:5]),\n",
    "                         np.hstack(images[5:10]),\n",
    "                         np.hstack(images[10:15])])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b60bb-fd52-4c57-a54c-3d83eaecab56",
   "metadata": {},
   "source": [
    "DEFINE A FUNCTION THAT WILL PUT CUSTOM TEXT ON TOP OF AN IMAGE, IN A CERTAIN LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9140e8bd-1f6a-413f-8121-954c03a708e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text(image, text, position):\n",
    "    pt1 = position\n",
    "    pt2 = (pt1[0] + 10 + (len(text) * 22),\n",
    "          pt1[1] - 45)\n",
    "    cv2.rectangle(image,\n",
    "                 pt1,\n",
    "                 pt2,\n",
    "                 (255, 255, 255),\n",
    "                 -1)\n",
    "    cv2.putText(image, text,\n",
    "               position,\n",
    "               fontFace = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               fontScale = 1.3,\n",
    "               color = (0, 0, 0),\n",
    "               thickness = 4)\n",
    "\n",
    "    #cCREATEE THE  MOSAIC WITH BOTH THE ORIGINAL AND THE GENERATED IMAGES, \n",
    "    # LABEL EACH SUB-GRID, AND DISPLAY THE RESULT:\n",
    "\n",
    "    original = stack(original)\n",
    "    generated = stack(generated)\n",
    "\n",
    "    mosaic = np.vstack([original,\n",
    "                       generated])\n",
    "\n",
    "    mosaic = cv2.resize(mosaic, (860, 860),\n",
    "                       interpolation = cv2.INTER_AREA)\n",
    "    mosaic = cv2.cvtColor(mosaic, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    \n",
    "    add_text(mosaic, \"Original\", (50, 100))\n",
    "    add_text(mosaic, \"Generated\", (50, 520))\n",
    "\n",
    "    cv2.imshow(\"Mosaic\", mosaic)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b00087-da53-48db-9e85-184b69d904e8",
   "metadata": {},
   "source": [
    "LOAD FASHION-MNIST USING TENSORFLOW'S HANDY FUNCTION.\n",
    "WE WILL ONLY KEEP THE IMAGES SINCE THE LABELS ARE UNNECESSARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1a4217-8426-4954-8c04-9b7cb705706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcee59b-ff0d-4257-9458-d3bc80aac262",
   "metadata": {},
   "source": [
    "NORMALIZE THE IMAGES AND ADD A SINGLE COLOR CHANNEL TO THE USING NP.EXPAND_DIMS():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b70c120-4ccd-49f1-8a22-9241fdb756ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis = -1)\n",
    "x_test = np.expand_dims(x_test, axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5aa3e9-0136-4a32-8ac2-ede1b2e20a93",
   "metadata": {},
   "source": [
    "GENERATE TWO TENSORS WITH THE SAME DIMENSIONS AS X_TRAIN AND X_TEST, RESPECTIVELY.\n",
    "    THESE WILL CORRSPOND TO RANDOM GAUSIAN NOISE THAT HAS A MEAN AND STANDARD DEVIATION EQUAL TO 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bdd82d9-35c9-4e61-b7b5-b7924829a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_noise = np.random.normal(loc = 0.5, scale = 0.5,\n",
    "                              size = x_train.shape)\n",
    "test_noise = np.random.normal(loc = 0.5, scale = 0.5,\n",
    "                             size = x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9f063-e556-40bb-9a65-9c1b98531e11",
   "metadata": {},
   "source": [
    "PURPOSELY DAMAGE BOTH X_TRAIN AND X_TEST BY ADDING TRAIN_NOISE AND TEST_NOISE, RESPECTIVELY.\n",
    "    MAKE SURE THAT THE VALUES REMAIN BETWEEN 0 AND 1USING NP.CLIP():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29912b7b-6dda-421e-8aec-715c9f10fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_noisy = np.clip(x_train + train_noise, 0, 1)\n",
    "x_test_noisy = np.clip(x_test + test_noise, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e0399-685b-4f0d-93be-85a224fbc93b",
   "metadata": {},
   "source": [
    "CREATE THE AUTENCODER AND COMPILE IT.\n",
    "WE WILL USE \"ADAM\" AS OUR OPTIMIZER AND \"MSE\" AS OUR LOSS FUNCTION, GIVEN THAT WE ARE INTERESTED\n",
    "IN REDUCING THE ERROR INSTEAD OF IMPROVING ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45be8ac7-6888-4e77-8f0f-3cb1ce4c383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, autoencoder = build_autoencoder(encoding_size = 128)\n",
    "autoencoder.compile(optimizer = \"adam\", loss = \"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97b80f-f2e9-4745-ae98-2345e032a7fc",
   "metadata": {},
   "source": [
    "FIT THE MODEL FOR 20 EPOCHS, ON BATCHES OF 1024 NOISY IMAGES AT A TIME.\n",
    "    NOTICE THAT THE FEATURES ARE THE NOISY IMAGES, WHILE THE LABELS OR TARGETA ARE \n",
    "THE ORIGINAL ONES, PROR TO BEING DAMAGED:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e19f29e9-ea21-4386-88e8-cdc2b9988e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 2s/step - loss: 0.0550 - val_loss: 0.1059\n",
      "Epoch 2/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - loss: 0.0247 - val_loss: 0.1132\n",
      "Epoch 3/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - loss: 0.0203 - val_loss: 0.1156\n",
      "Epoch 4/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - loss: 0.0184 - val_loss: 0.1135\n",
      "Epoch 5/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - loss: 0.0172 - val_loss: 0.1053\n",
      "Epoch 6/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - loss: 0.0165 - val_loss: 0.0927\n",
      "Epoch 7/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - loss: 0.0159 - val_loss: 0.0743\n",
      "Epoch 8/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - loss: 0.0156 - val_loss: 0.0600\n",
      "Epoch 9/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - loss: 0.0151 - val_loss: 0.0464\n",
      "Epoch 10/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - loss: 0.0149 - val_loss: 0.0355\n",
      "Epoch 11/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - loss: 0.0146 - val_loss: 0.0256\n",
      "Epoch 12/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - loss: 0.0144 - val_loss: 0.0195\n",
      "Epoch 13/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - loss: 0.0141 - val_loss: 0.0164\n",
      "Epoch 14/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - loss: 0.0140 - val_loss: 0.0173\n",
      "Epoch 15/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - loss: 0.0138 - val_loss: 0.0162\n",
      "Epoch 16/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - loss: 0.0137 - val_loss: 0.0147\n",
      "Epoch 17/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 1s/step - loss: 0.0135 - val_loss: 0.0160\n",
      "Epoch 18/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - loss: 0.0134 - val_loss: 0.0146\n",
      "Epoch 19/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - loss: 0.0133 - val_loss: 0.0143\n",
      "Epoch 20/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - loss: 0.0132 - val_loss: 0.0149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14672170b10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1024\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "               epochs = EPOCHS,\n",
    "               batch_size = BATCH_SIZE,\n",
    "               shuffle = True,\n",
    "               validation_data = (x_test_noisy, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5da9ab-6b5c-4a7c-a02f-d1f9cc5b046d",
   "metadata": {},
   "source": [
    "MAKE  PREDICTIONS WITH THE TRAINED MODEL.\n",
    "    RESHAPE BOTH THE NOISY AND GENERATED IMAGES BACK TO 28X28, AND SCALE THEM UP TO THE [0, 255] RANGE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "094791d6-ac38-4696-8596-ccc1464e2c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = autoencoder.predict(x_test)\n",
    "\n",
    "original_shape = (x_test_noisy.shape[0], 28, 28)\n",
    "predictions = predictions.reshape(original_shape)\n",
    "x_test_noisy = x_test_noisy.reshape(original_shape)\n",
    "\n",
    "predictions = (predictions * 255.0).astype(\"uint8\")\n",
    "x_test_noisy = (x_test_noisy * 255.0).astype(\"uint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbdb194e-4f43-4a6b-a4b7-23e29eb4b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_original_vs_generated(x_test_noisy, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160ea20a-7b61-4cd6-939c-b74dd59dbcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
